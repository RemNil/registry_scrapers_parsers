{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "def get_url(url):\n",
    "    response = get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from time import sleep\n",
    "import os\n",
    "import re\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "platform = platform.platform()\n",
    "cwd = os.getcwd()\n",
    "download_path = os.path.join(cwd,'CRiS Downloads')\n",
    "#adjust this to fit your specific file structure \n",
    "parent = str(Path(cwd).parents[0]) \n",
    "\n",
    "if \"Darwin\" in platform:\n",
    "    chrome_driver = os.path.join(parent, 'Drivers', 'chromedriver')\n",
    "elif \"Windows\" in platform:\n",
    "    chrome_driver = os.path.join(parent, 'Drivers', 'chromedriver.exe')\n",
    "else:\n",
    "    print(\"No OS/Chromedriver match. OS: {}\".format(platform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the last trial ID using Selenium\n",
    "\n",
    "chromeOptions = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\" : download_path,\n",
    "        'disk-cache-size': 4096,\n",
    "        'safebrowsing.enabled': 'false'}\n",
    "chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver, options=chromeOptions)\n",
    "driver.get('https://www.drks.de/drks_web/setPage.do?page=1')\n",
    "select = Select(driver.find_element_by_name('criteria.sortField'))\n",
    "select.select_by_value('trial_drks_id')\n",
    "driver.find_element_by_xpath(\"//a[@href ='setPage.do?page=last']\").click()\n",
    "thing = (driver.find_element_by_xpath(\"//tbody\")).text\n",
    "driver.quit()\n",
    "\n",
    "split = thing.split('|')\n",
    "last_id = re.findall(re.compile('DRKS\\d{8}'), split[-1])[0]\n",
    "\n",
    "#Then go an make a list of all possibl trial IDs from 1 to the max\n",
    "\n",
    "last_n = int(last_id[-5:])\n",
    "\n",
    "id_num_list = list(range(1,last_n+1))\n",
    "\n",
    "trial_ids = []\n",
    "for num in id_num_list:\n",
    "    trial_id = 'DRKS' + '0' * (8 - len(str(num))) + str(num)\n",
    "    trial_ids.append(trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(last_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all functions to proccess data\n",
    "\n",
    "def does_it_exist(soup, element, e_class, n_e=False):\n",
    "    if not n_e:\n",
    "        location = soup.find(element, class_=e_class).text.strip()\n",
    "    elif n_e:\n",
    "        location = soup.find(element, class_=e_class).next_element.next_element.next_element.next_element.strip()\n",
    "    if location == '[---]*':\n",
    "        field = None\n",
    "    else:\n",
    "        field = location\n",
    "    return field\n",
    "\n",
    "def get_contact_info(soup, address_num):\n",
    "\n",
    "    test_dict = {}\n",
    "\n",
    "    addresses = soup.find_all('ul', class_='trial_address')\n",
    "    if address_num > len(addresses)-1:\n",
    "        test_dict = None\n",
    "    else:\n",
    "        uls = addresses[address_num].find_all('li')\n",
    "        counter = 1\n",
    "        ad_str = ''\n",
    "        for u in uls:\n",
    "            u = u.text.strip()\n",
    "            u = re.sub('\\s+', ' ', u)\n",
    "            if counter < len(uls):\n",
    "                ad_str = ad_str + u + \" \"\n",
    "            else:\n",
    "                as_str = ad_str + u\n",
    "\n",
    "        test_dict['sponsor_address'] = ad_str\n",
    "\n",
    "        a_labels = ['phone', 'fax', 'email', 'url']\n",
    "        contacts = soup.find_all('ul', class_='trial_address_contact')\n",
    "        ul = contacts[address_num].find_all('li')\n",
    "        for u, l in zip(ul, a_labels):\n",
    "            li_t = u.text.strip()\n",
    "            li_t = re.sub('\\s+', '', li_t)\n",
    "            if 'URL:' in li_t:\n",
    "                li_list = li_t.split('RL:')\n",
    "            else:\n",
    "                li_list = li_t.split(':')\n",
    "            if li_list[1] == '[---]*':\n",
    "                test_dict[l] = None\n",
    "            else:\n",
    "                test_dict[l] = li_list[1]\n",
    "    return test_dict\n",
    "\n",
    "def trial_info(soup):    \n",
    "    t_d = {}\n",
    "\n",
    "    t_d['drks_id'] = soup.find('li', class_='drksId').next_element.next_element.next_element.next_element.strip()\n",
    "\n",
    "    if soup.find('div', class_='retrospective-hint'):\n",
    "        t_d['registration_status'] = 'retrospective'\n",
    "    else:\n",
    "        t_d['registration_status'] = 'prospetive'\n",
    "\n",
    "    st_class = ['state', 'deadline']\n",
    "    st_labels = ['recruitment_status', 'study_closing_date']\n",
    "    for lab, s_class in zip(st_labels, st_class):\n",
    "        t_d[lab] = does_it_exist(soup, 'li', s_class, n_e=True)\n",
    "\n",
    "    t_d['trial_acronym'] = does_it_exist(soup, 'p', 'acronym')\n",
    "    t_d['trial_url'] = does_it_exist(soup, 'p', 'trial_url')\n",
    "    t_d['registration_date'] = does_it_exist(soup, 'li', 'firstDrksPublishDate', n_e = True)\n",
    "    t_d['partner_registration_date'] = does_it_exist(soup, 'li', 'firstPartnerPublishDate', n_e = True)\n",
    "    t_d['investigator_sponsored_or_initiated_trial'] = does_it_exist(soup, 'li', 'investorInitiated', n_e = True)\n",
    "    t_d['ethics_info'] = {'ethics_status': does_it_exist(soup, 'li', 'ethicCommitteeVote', n_e=True),\n",
    "                          'ethics_committee': does_it_exist(soup, 'li', 'ethicCommissionNumber', n_e=True)}\n",
    "\n",
    "    s_id_list = []\n",
    "    ul = soup.find('ul', class_='secondaryIDs').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        s_id_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            s_id_dict = {}\n",
    "            s_id_dict['id_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            li_t = u.next_element.next_element.next_element.next_element.strip()\n",
    "            li_t = re.sub('\\n', '|', li_t)\n",
    "            li_t = re.sub('\\s+', '', li_t).replace('(','').replace(')','')\n",
    "            id_info = li_t.split('|')\n",
    "            if len(id_info) > 1:   \n",
    "                s_id_dict['registry'] = id_info[1]\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            else:\n",
    "                s_id_dict['registry'] = None\n",
    "                s_id_dict['secondary_id'] = id_info[0]\n",
    "            s_id_list.append(s_id_dict)\n",
    "    t_d['secondary_ids'] = s_id_list\n",
    "\n",
    "    h_list = []\n",
    "    ul = soup.find('ul', class_='health-conditions').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        h_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            h_dict = {}\n",
    "            h_dict['identifier_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            li_t = u.next_element.next_element.next_element.next_element.strip()\n",
    "            li_t = re.sub('\\s+', '', li_t)\n",
    "            h_dict['condition'] = li_t\n",
    "            h_list.append(h_dict)\n",
    "    t_d['health_condition_problem'] = h_list \n",
    "\n",
    "    i_list = []\n",
    "    ul = soup.find('ul', class_='interventions').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        i_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            i_dict = {}\n",
    "            i_dict['arm'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            li_t = u.next_element.next_element.next_element.next_element.strip()\n",
    "            i_dict['intervention_type'] = li_t\n",
    "            i_list.append(i_dict)\n",
    "    t_d['interventions'] = i_list\n",
    "\n",
    "    char_class = ['type', 'typeNotInterventional', 'allocation', 'masking maskingType', 'maskingWho', 'control', 'purpose',\n",
    "                  'assignment', 'phase', 'offLabelDrugUse']\n",
    "    char_labels = ['study_type', 'study_type_non_int', 'allocation', 'blinding', 'who_blinded', 'control', 'purpose', \n",
    "                  'assignemnt', 'phase', 'off_label_drug']\n",
    "    char_dict = {}\n",
    "    for lab, c_class in zip(char_labels, char_class):\n",
    "        char_dict[lab] = does_it_exist(soup, 'li', c_class, n_e=True)\n",
    "    t_d['trial_charcteristics'] = char_dict\n",
    "\n",
    "    t_d['primary_outcomes'] = does_it_exist(soup, 'p', 'primaryEndpoint')\n",
    "\n",
    "    t_d['secondary_outcomes'] = does_it_exist(soup, 'p', 'secondaryEndpoints')\n",
    "\n",
    "    c_list = []\n",
    "    ul = soup.find('ul', class_='recruitmentCountries').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        c_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            li_t = u.text.strip()\n",
    "            li_t = re.sub('\\s+', '', li_t)\n",
    "            li_list = li_t.split(':')\n",
    "        c_list.append(li_list[1])\n",
    "    t_d['countries'] = c_list\n",
    "\n",
    "    l_list = []\n",
    "    ul = soup.find('ul', class_='recruitmentLocations').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        l_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            l_dict = {}\n",
    "            loc = u.next_element.next_element.text.strip()\n",
    "            loc = re.sub('\\s+', '', loc).replace(',', ', ')\n",
    "            l_dict['location'] = loc\n",
    "            l_dict['location_type'] = ul[0].next_element.strip()\n",
    "            l_list.append(l_dict)\n",
    "    t_d['recuitment_locations'] = l_list\n",
    "\n",
    "    rec_class = ['running', 'schedule', 'targetSize', 'monocenter', 'national']\n",
    "    rec_labels = ['planned_actual', 'date_of_first_enrollment', 'target_sample_size', 'mono_multi_center', \n",
    "                  'national_international']\n",
    "    rec_dict = {}\n",
    "    for lab, r_class in zip(rec_labels, rec_class):\n",
    "        rec_dict[lab] = does_it_exist(soup, 'li', r_class, n_e=True)\n",
    "    t_d['recruitment_info'] = rec_dict\n",
    "\n",
    "    inc_class = ['gender', 'minAge', 'maxAge']\n",
    "    inc_labels = ['gender', 'min_age', 'max_age']\n",
    "    inc_dict = {}\n",
    "    for lab, i_class in zip(inc_labels, inc_class):\n",
    "        inc_dict[lab] = does_it_exist(soup, 'li', i_class, n_e=True)\n",
    "    t_d['inclusion_criteria'] = rec_dict\n",
    "\n",
    "    t_d['additional_inclusion_criteria'] = does_it_exist(soup, 'p', 'inclusionAdd')\n",
    "\n",
    "    t_d['exclusion_criteria'] = does_it_exist(soup, 'p', 'publicSummary')\n",
    "\n",
    "    t_d['lay_summary'] = does_it_exist(soup, 'p', 'exclusion')\n",
    "\n",
    "    t_d['scientific_summary'] = does_it_exist(soup, 'p', 'scientificSynopsis')\n",
    "\n",
    "    t_d['primary_sponsor_contact'] = get_contact_info(soup, 0)\n",
    "\n",
    "    t_d['scientific_contact'] = get_contact_info(soup, 1)\n",
    "\n",
    "    t_d['public_contact'] = get_contact_info(soup, 2)\n",
    "\n",
    "    t_d['funder_contact'] = get_contact_info(soup, 3)\n",
    "\n",
    "    docs_list = []\n",
    "    ul = soup.find('ul', class_='publications').find_all('li')\n",
    "    if ul[0].text == '[---]*':\n",
    "        docs_list = None\n",
    "    else:\n",
    "        for u in ul:\n",
    "            doc_dict = {}\n",
    "            doc_dict['document_type'] = u.next_element.next_element.next_element.strip().replace(':','')\n",
    "            if u.find('a'):\n",
    "                doc_dict['link_to_document'] = u.find('a').get('href')\n",
    "            else:\n",
    "                doc_dict['link_to_document'] = None\n",
    "            docs_list.append(doc_dict)\n",
    "    t_d['results_publications_documents'] = docs_list\n",
    "\n",
    "    return t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.drks.de/drks_web/navigate.do?navigationId=trial.HTML&TRIAL_ID='\n",
    "\n",
    "error_text = ['Due to an error of the data management, this study was inadvertently registered incompletely.',\n",
    "              'This study has been imported from ClinicalTrials.gov inadvertently, although it had been registered with DRKS before.']\n",
    "\n",
    "headers = ['drks_id', 'registration_status', 'recruitment_status', 'study_closing_date', 'trial_acronym', 'trial_url',\n",
    " 'registration_date', 'partner_registration_date', 'investigator_sponsored_or_initiated_trial', 'ethics_info', 'secondary_ids',\n",
    " 'health_condition_problem', 'interventions', 'trial_charcteristics', 'primary_outcomes', 'secondary_outcomes', 'countries',\n",
    " 'recuitment_locations', 'recruitment_info', 'inclusion_criteria', 'additional_inclusion_criteria', 'exclusion_criteria',\n",
    " 'lay_summary', 'scientific_summary', 'primary_sponsor_contact', 'scientific_contact', 'public_contact', 'funder_contact',\n",
    " 'results_publications_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "with open('drks_trials.csv', 'w', newline='', encoding='utf-8') as drks_csv:\n",
    "    writer = csv.DictWriter(drks_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for tid in tqdm(trial_ids[]):\n",
    "        soup = get_url(base_url + tid)\n",
    "        if soup.find('ul', {'class':'errors'}) or soup.find('div', class_='trial').text in error_text:\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                writer.writerow(trial_info(soup))\n",
    "            except Exception as e:\n",
    "                import sys\n",
    "                raise type(e)(str(e) + '\\n' + 'Error trial: {}'.format(tid)).with_traceback(sys.exc_info()[2])\n",
    "end_time = time()\n",
    "print('Scrape Finished in {} minues'.format(round((end_time-start_time) / 60),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
