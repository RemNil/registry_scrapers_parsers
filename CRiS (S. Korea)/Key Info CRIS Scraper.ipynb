{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "def get_url(url):\n",
    "    response = get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def get_elements():\n",
    "    return((driver.find_elements_by_xpath(\"//p[@class='searchtbp1']//a\")))\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import re\n",
    "chrome_driver = 'C:/Users/ndevito/Dropbox/Python projects/Drivers/chromedriver.exe'\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "platform = platform.platform()\n",
    "cwd = os.getcwd()\n",
    "download_path = os.path.join(cwd,'Rebec Downloads')\n",
    "#adjust this to fit your specific file structure \n",
    "parent = str(Path(cwd).parents[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Darwin\" in platform:\n",
    "    chrome_driver = os.path.join(parent, 'Drivers', 'chromedriver')\n",
    "elif \"Windows\" in platform:\n",
    "    chrome_driver = os.path.join(parent, 'Drivers', 'chromedriver.exe')\n",
    "else:\n",
    "    print(\"No OS/Chromedriver match. OS: {}\".format(platform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chromeOptions = webdriver.ChromeOptions()\n",
    "    prefs = {'disk-cache-size': 4096,\n",
    "            'safebrowsing.enabled': 'false'}\n",
    "    chromeOptions.add_experimental_option(\"prefs\",prefs)\n",
    "    driver = webdriver.Chrome(executable_path=chrome_driver, options=chromeOptions)\n",
    "    driver.get('http://cris.nih.go.kr/cris/en/search/basic_search.jsp')\n",
    "    select = Select(driver.find_element_by_id('rowcount'))\n",
    "    select.select_by_value('50')\n",
    "    driver.find_element_by_xpath(\"//input[@alt='Search']\").click()\n",
    "    total_trials = (driver.find_element_by_xpath(\"//strong[@class='searchtbtnb']\")).text\n",
    "    onclick_el = (driver.find_elements_by_xpath(\"//a[@href='#paging']\"))[-1].get_attribute('onclick')\n",
    "    max_page = onclick_el[onclick_el.find(\"(\")+1:onclick_el.find(\")\")]\n",
    "\n",
    "    url_suff_list = []\n",
    "    pattern = re.compile(r'\\d+$')\n",
    "    for a in get_elements():\n",
    "        url_suff_list.append(re.search(pattern, a.get_attribute('href')).group())\n",
    "    next_page = 2\n",
    "    while next_page < int(max_page)+1:\n",
    "        (driver.find_element_by_xpath(\"//a[@onclick ='return jf_page({});']\".format(next_page))).click()\n",
    "        for a in get_elements():\n",
    "            url_suff_list.append(re.search(pattern, a.get_attribute('href')).group())\n",
    "        next_page += 1\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    driver.quit()\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(total_trials) == len(url_suff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_cris_url = 'https://cris.nih.go.kr/cris/en/search/search_result_st01.jsp?seq='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "\n",
    "def str_to_date(datestr):  \n",
    "    is_defaulted_date = False\n",
    "    #this fixes a specific issie with one trial with an impossible date for a primart completion date (31 June 2017)\n",
    "    #https://cris.nih.go.kr/cris/en/search/search_result_st01.jsp?seq=7625\n",
    "    #based on the provided full completion date in that study, we have defaulted to 30 June 2017\n",
    "    if datestr == '2017-06-31':\n",
    "        parsed_date = date(2017,6,30)\n",
    "        is_defaulted_date = True\n",
    "    #this fixes an issue with a trial with a bad date format (no hyphens) in the start date field\n",
    "    #https://cris.nih.go.kr/cris/en/search/search_result_st01.jsp?seq=3990\n",
    "    elif datestr == '20140508':\n",
    "        parsed_date = date(2014,5,8)\n",
    "        is_defaulted_date = True\n",
    "    elif datestr not in ['-', 'Actual', 'Anticipated', '']:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(datestr, '%Y-%m-%d').date()\n",
    "        except ValueError:\n",
    "            parsed_date = datetime.strptime(datestr, '%Y-%m').date() + relativedelta(months=+1) - timedelta(days=1)\n",
    "            is_defaulted_date = True\n",
    "    else:\n",
    "        parsed_date = None\n",
    "    return (parsed_date, is_defaulted_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7c6f19ce174fa085cbe246633581cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1082), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_cris_trials = []\n",
    "for suff in tqdm(url_suff_list):\n",
    "    t_d = {}\n",
    "    soup = get_url(default_cris_url + suff)\n",
    "    t_d['trial_id'] = soup.find(text=re.compile(r'CRIS Registration Number')).findNext('td').text.strip()\n",
    "    t_d['recruitment_status'] = soup.find(text=re.compile(r'Overall Recruitment Status')).findNext('td').text.strip()\n",
    "    for x in ['Terminated', 'Suspended', 'Withdrawn']:\n",
    "        if x in t_d['recruitment_status']:\n",
    "            t_d['recruitment_status'] = x\n",
    "            break \n",
    "    t_d['start_date'] = str_to_date(soup.find(text=re.compile(r'Date of First Enrollment')).findNext('td').text.strip().split(',')[0].strip())[0]\n",
    "    t_d['primary_completion_date'] = str_to_date(soup.find(text=re.compile(r'Primary Completion Date')).findNext('td').text.strip().split(',')[0].strip())[0]\n",
    "    t_d['study_completion_date'] = str_to_date(soup.find(text=re.compile(r'Study Completion Date')).findNext('td').text.strip().split(',')[0].strip())[0]\n",
    "    results_info = {}\n",
    "    results_registered = {}\n",
    "    if soup.find(text=re.compile(r'Result Registerd')).findNext('td').text.strip() == 'No' or not soup.find(text=re.compile(r'Result Registerd')).findNext('td').text.strip():\n",
    "        results_registered['has_results'] = 'No'\n",
    "        results_info['results_registered'] = results_registered\n",
    "    else:\n",
    "        for t in soup.find(text=re.compile(r'Result Registerd')).findNext('td').text.strip().split('\\r\\n\\t'):\n",
    "            if t.strip() == '':\n",
    "                continue\n",
    "            elif t.strip() == 'Yes':\n",
    "                results_registered['has_results'] = 'Yes'\n",
    "            elif t.strip() == 'Published':\n",
    "                results_registered['published'] = 'Yes'\n",
    "            elif t.strip() == 'Results Upload':\n",
    "                results_registered['uploaded'] = 'Yes'\n",
    "        results_info['results_registered'] = results_registered\n",
    "\n",
    "        publications = {}\n",
    "        if soup.find(text=re.compile(r'Number of Publication')).findNext('td').text.strip() == '0':\n",
    "            pass\n",
    "        else:\n",
    "            publications['number_of_pubs'] = soup.find(text=re.compile(r'Number of Publication')).findNext('td').text.strip()\n",
    "            for pub in list(range(1,int(soup.find(text=re.compile(r'Number of Publication')).findNext('td').text.strip())+1)):\n",
    "                publications['publication_{}'.format(pub)] = soup.find(text=re.compile(r'Publications {}'.format(pub))).findNext('td').text.strip().replace('\\t','').replace('\\xa0','').replace('\\r','').replace('\\n','')\n",
    "            results_info['publications'] = publications\n",
    "\n",
    "        if soup.find(text=re.compile(r'Results Upload')).findNext('a').get('href'):\n",
    "            results_info['uploaded_document'] = 'https://cris.nih.go.kr/cris' + soup.find(text=re.compile(r'Results Upload')).findNext('a').get('href').replace('../..','')\n",
    "\n",
    "        if soup.find(text=re.compile(r'Date of Posting Results')).findNext('td').text.strip():\n",
    "            results_info['results_posted_date'] = str_to_date(soup.find(text=re.compile(r'Date of Posting Results')).findNext('td').text.strip())[0]\n",
    "\n",
    "        if soup.find(text=re.compile(r'Protocol URL or File Upload')).findNext('td').text.strip():\n",
    "            if '../..' in soup.find(text=re.compile(r'Protocol URL or File Upload')).findNext('a').get('href'):\n",
    "                results_info['protocol'] = 'https://cris.nih.go.kr/cris' + soup.find(text=re.compile(r'Protocol URL or File Upload')).findNext('a').get('href').replace('../..','')\n",
    "            else:\n",
    "                results_info['protocol'] = soup.find(text=re.compile(r'Protocol URL or File Upload')).findNext('a').get('href')\n",
    "\n",
    "        if soup.find(text=re.compile(r'Brief Summary')).findNext('td').text.strip():\n",
    "            results_info['brief_summary'] = soup.find(text=re.compile(r'Brief Summary')).findNext('td').text.strip()\n",
    "    t_d['results_info'] = results_info\n",
    "    all_cris_trials.append(t_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4430"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_cris_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import date\n",
    "import json\n",
    "\n",
    "def cris_csv():\n",
    "    with open('cris_scrape - ' + str(date.today()) + '.csv','w', newline = '')as cris_csv:\n",
    "        writer=csv.writer(cris_csv)\n",
    "        for val in all_cris_trials:\n",
    "            writer.writerow([json.dumps(val, indent=4, sort_keys=True, default=str)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cris_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
