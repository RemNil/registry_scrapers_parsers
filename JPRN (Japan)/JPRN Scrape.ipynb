{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_url('https://rctportal.niph.go.jp/en/result?t=chiken&l=50&s=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_count = soup.find('div', {'id': 'result-counts'}).find('span').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c86495464e4a6990eb1e05f3ce6040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scrape Finished in 0 minues\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "all_links = []\n",
    "#pages = list(range(0,int(trial_count)+1,50))\n",
    "pages = list(range(0,400+1,50))\n",
    "for p in tqdm(pages):\n",
    "    soup = get_url('https://rctportal.niph.go.jp/en/result?t=chiken&l=50&s={}'.format(str(p)))\n",
    "    hrefs = soup.find('div', {'class': 'chikentbl'}).find_all('a')\n",
    "    hrefs = hrefs[10:]\n",
    "    for h in hrefs:\n",
    "        all_links.append(h.get('href'))\n",
    "end_time = time()\n",
    "print('Scrape Finished in {} minues'.format(round((end_time-start_time) / 60),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jprn_prefix = 'https://rctportal.niph.go.jp'\n",
    "\n",
    "def get_field(soup, field):\n",
    "    x = soup.find(text=field).parent.parent.find('td').text\n",
    "    return x\n",
    "\n",
    "def get_table(soup, table, t_dict):\n",
    "    tab = soup.find('table', {'class': table}).find_all('th')\n",
    "    for t in tab:\n",
    "        t_dict[t.text.lower().replace(' ','_')] = get_field(soup, t.text)\n",
    "\n",
    "def get_common(table, soup):\n",
    "    temp_dict = {}\n",
    "    fields = table.find_all('th')\n",
    "    for f in fields:\n",
    "        temp_dict[f.text.lower().replace(' ','_')] = get_field(soup, f.text)\n",
    "    return temp_dict\n",
    "\n",
    "def get_row(soup):\n",
    "    t_d = {}\n",
    "    t_d['trial_id'] = re.search(r\":\\s(.*)\", soup.find('div', {'class': 'japicid'}).text).group(1)\n",
    "    t_d['registered_date'] = re.search(r\":\\s?(.*)\", soup.find('div', {'class': 'signupdate'}).text).group(1)\n",
    "    get_table(soup, 'basic', t_d)\n",
    "    t_d['original_registry_link'] = soup.find('div', {'class','syousaibtn'}).find('a').get('rel')[0]\n",
    "    get_table(soup, 'test', t_d)\n",
    "    get_table(soup, 'target', t_d)\n",
    "    tables = soup.find_all('table', {'class': 'common'})\n",
    "    t_d['sponsor_info'] = get_common(tables[0], soup)\n",
    "    ids_field = tables[1].find('th').parent.parent.find('td')\n",
    "    if ',' in ids_field:\n",
    "        t_d['secondary_ids'] = ids_field.text.split(',')\n",
    "    else:\n",
    "        t_d['secondary_ids'] = ids_field.text\n",
    "\n",
    "    c1 = tables[2].find_all('tr')[1:6]\n",
    "    c2 = tables[2].find_all('tr')[7:]\n",
    "    contact_fields = ['name', 'address', 'telephone', 'email', 'affiliation']\n",
    "\n",
    "    public_contact = {}\n",
    "    for c, cf in zip(c1, contact_fields):\n",
    "        public_contact[cf] = c.find_all('td')[1].text\n",
    "    t_d['public_contact'] = public_contact\n",
    "\n",
    "    scientific_contact = {}\n",
    "    for c, cf in zip(c2, contact_fields):\n",
    "        scientific_contact[cf] = c.find_all('td')[1].text\n",
    "    t_d['scientific_contact'] = scientific_contact\n",
    "    \n",
    "    t_d['ethics_info'] = get_common(tables[3], soup)\n",
    "    t_d['results_info'] = get_common(tables[4], soup)\n",
    "    t_d['ipd_sharing'] = get_common(tables[5], soup)\n",
    "    return t_d\n",
    "\n",
    "headers = ['trial_id', 'registered_date', 'public_title', 'scientific_title', 'recruitment_status', \n",
    "           'health_condition(s)_or_problem(s)_studied', 'study_type', 'phase', 'study_design', 'intervention(s)', \n",
    "           'sample_size', 'date_of_first_enrollment', 'completion_date', 'countries_of_recruitment', 'original_registry_link',\n",
    "           'primary_outcome', 'secondary_outcome', 'age_minimum', 'age_maximum', 'gender', 'include_criteria',\n",
    "           'exclude_criteria', 'sponsor_info', 'secondary_ids', 'public_contact', 'scientific_contact', 'ethics_info',\n",
    "           'results_info', 'ipd_sharing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0c25e0d4764568bda653049b30ac60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scrape Finished in 7 minues\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "with open('jprn_trials.csv', 'w', newline='', encoding='utf-8') as jprn_csv:\n",
    "    writer = csv.DictWriter(jprn_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    request = 0\n",
    "    for l in tqdm(all_links):\n",
    "        soup = get_url(jprn_prefix + l)\n",
    "        trial_info = get_row(soup)\n",
    "        writer.writerow(trial_info)\n",
    "end_time = time()\n",
    "print('Scrape Finished in {} minues'.format(round((end_time-start_time) / 60),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/43842206/how-to-filter-a-pandas-dataframe-by-dict-column"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
