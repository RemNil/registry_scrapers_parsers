{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tables(soup, sec_name, labels, secondary_labels=None):\n",
    "    sect_list = []\n",
    "    if not soup.find('h3', text=re.compile(sec_name)).findNext('div').findNext('div', {'class':'empty-section'}):\n",
    "        for sub_s in soup.find('h3', text=re.compile(sec_name)).parent.find_all('div', {'class': 'subsection'}):\n",
    "            sect_group = {}\n",
    "            counter = 0\n",
    "            for dd in sub_s.find_all('dd'):\n",
    "                if dd.find('div', {'class': 'well contact-card'}):\n",
    "                    sect_dict = {}\n",
    "                    for l, t in zip(secondary_labels, dd.find_all('div', {'class': 'col-md-9 contact-value'})):\n",
    "                        sect_dict[l] = t.text.strip()\n",
    "                    sect_group[labels[counter]] = sect_dict\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    sect_group[labels[counter]] = dd.text.strip()\n",
    "                    counter+=1\n",
    "            sect_list.append(sect_group)\n",
    "    else:\n",
    "        sect_list = []\n",
    "    return sect_list\n",
    "\n",
    "def get_row(soup):\n",
    "    t_d = {}\n",
    "    t_d['trial_id'] = soup.find('span', text='IRCT registration number:').findNext('strong').text\n",
    "    t_d['trial_name'] = soup.find('div', {'class':'page-header'}).text.strip()\n",
    "    #dates in Y-M-D format with leading 0s for M and D\n",
    "    t_d['registration_date'] = soup.find('span', text='Registration date:').findNext('strong').text[:10]\n",
    "    t_d['registration_timing'] = soup.find('span', text='Registration timing:').findNext('strong').text\n",
    "    t_d['last_updated_date'] = soup.find('span', text='Last update:').findNext('strong').text[:10]\n",
    "    t_d['number_of_updates'] = soup.find('span', text='Update count:').findNext('strong').text\n",
    "    if soup.find('dt', text=re.compile('Summary')):\n",
    "        t_d['trial_summary'] = soup.find('dt', text=re.compile('Summary')).findNext('div').text.strip()\n",
    "    else:\n",
    "        t_d['trial_summary'] = None\n",
    "\n",
    "    registrant_info = {}\n",
    "    registrant_info['name'] = soup.find('dt', text=re.compile('Registrant information')).findNext('div').find('strong', text='Name').findNext('div').text.strip()\n",
    "    registrant_info['organization'] = soup.find('dt', text=re.compile('Registrant information')).findNext('div').find('strong', text='Name of organization / entity').findNext('div').text.strip()\n",
    "    registrant_info['country'] = soup.find('dt', text=re.compile('Registrant information')).findNext('div').find('strong', text='Country').findNext('div').text.strip()\n",
    "    registrant_info['phone'] = soup.find('dt', text=re.compile('Registrant information')).findNext('div').find('strong', text='Phone').findNext('div').text.strip()\n",
    "    registrant_info['email'] = soup.find('dt', text=re.compile('Registrant information')).findNext('div').find('strong', text='Email address').findNext('div').text.strip()\n",
    "    t_d['registrant_info'] = registrant_info\n",
    "    t_d['trial_status'] = soup.find('dt', text=re.compile('Recruitment status')).findNext('strong').text.strip()\n",
    "    t_d['funding_source'] = soup.find('dt', text=re.compile('Funding source')).findNext('div').text.strip().replace('ِِ','')\n",
    "    t_d['expected_start_date'] = soup.find('dt', text=re.compile('Expected recruitment start date')).findNext('dd').text.strip()[:10]\n",
    "    t_d['expected_recruitment_end_date'] = soup.find('dt', text=re.compile('Expected recruitment end date')).findNext('dd').text.strip()[:10]\n",
    "    t_d['actual_start_date'] = soup.find('dt', text=re.compile('Actual recruitment start date')).findNext('dd').text.strip()[:10]\n",
    "    t_d['actual_recruitment_end_date'] = soup.find('dt', text=re.compile('Actual recruitment end date')).findNext('dd').text.strip()[:10]\n",
    "    t_d['trial_completion_date'] = soup.find('dt', text=re.compile('Trial completion date')).findNext('dd').text.strip()[:10]\n",
    "    t_d['scientific_title'] = soup.find('dt', text=re.compile('Scientific title')).findNext('dd').text.strip()\n",
    "    t_d['public_title'] = soup.find('dt', text=re.compile('Public title')).findNext('dd').text.strip()\n",
    "    t_d['trial_purpose'] = soup.find('dt', text=re.compile('Purpose')).findNext('dd').text.strip()\n",
    "    t_d['inclusion_exclusion_criteria'] = soup.find('dt', text=re.compile('Inclusion/Exclusion criteria')).findNext('dd').text.strip()\n",
    "    t_d['subject_age'] = soup.find('dt', text=re.compile('Age')).findNext('dd').text.strip()\n",
    "    t_d['subject_gender'] = soup.find('dt', text=re.compile('Gender')).findNext('dd').text.strip()\n",
    "    if soup.find('dt', text=re.compile('Phase')).findNext('dd').text.strip() == 'N/A':\n",
    "        t_d['phase'] = 'N/A'\n",
    "    else:\n",
    "        t_d['phase'] = 'phase ' + soup.find('dt', text=re.compile('Phase')).findNext('dd').text.strip()\n",
    "    t_d['masking'] =  soup.find('dt', text=re.compile('Groups that have been masked')).findNext('dd').text.strip()\n",
    "    t_d['sample_size'] = soup.find('dt', text=re.compile('Sample size')).findNext('dd').findNext('strong').text.strip()\n",
    "    t_d['randomization'] = soup.find('dt', text=re.compile(\"Randomization \\(investigator's opinion\\)\")).findNext('dd').text.strip()\n",
    "    t_d['randomization_description'] = soup.find('dt', text=re.compile(\"Randomization description\")).findNext('dd').text.strip()\n",
    "    t_d['blinding'] = soup.find('dt', text=re.compile(\"Blinding \\(investigator's opinion\\)\")).findNext('dd').text.strip()\n",
    "    t_d['blinding_description'] = soup.find('dt', text=re.compile(\"Blinding description\")).findNext('dd').text.strip()\n",
    "    t_d['placebo'] = soup.find('dt', text=re.compile(\"Placebo\")).findNext('dd').text.strip()\n",
    "    t_d['assignment'] = soup.find('dt', text=re.compile(\"Assignment\")).findNext('dd').text.strip()\n",
    "    t_d['other_design_features'] = soup.find('dt', text=re.compile(\"Other design features\")).findNext('dd').text.strip()\n",
    "    t_d['secondary_ids'] = mult_tables(soup, 'Secondary Ids', ['registry_name', 'secondary_id', 'registration_date'])\n",
    "    t_d['ethics_information'] = mult_tables(soup, 'Ethics committees', ['ethics_committee', 'approval_date', 'reference_number'], secondary_labels=['committee_name', 'address', 'city', 'postal_code'])\n",
    "    t_d['health_conditions'] = mult_tables(soup,'Health conditions studied', ['condition_description', 'icd_10_code', 'icd_10_description'])\n",
    "    t_d['primary_outcomes'] = mult_tables(soup,'Primary outcomes', ['description', 'timepoint', 'measurement_method'])\n",
    "    t_d['secondary_outcomes'] = mult_tables(soup,'Secondary outcomes', ['description', 'timepoint', 'measurement_method'])\n",
    "    t_d['intervention_groups'] = mult_tables(soup,'Intervention groups', ['description', 'category'])\n",
    "    t_d['recruitement_centers'] = mult_tables(soup,'Recruitment centers', ['center_info'], secondary_labels=['name', 'responsible_party', 'address', 'city'])\n",
    "    t_d['sponsor_funding_sources'] = mult_tables(soup,'Sponsors / Funding sources', ['sponsor_contact', 'grant_name', 'grant_number', 'funding_same_as_sponsor', \n",
    "                                                    'funder_title', 'proportion_of_funding', 'public_of_private', \n",
    "                                                    'domestic_or_foreign', 'forgeign_source_category', 'country_of_origin', 'org_type'], \n",
    "                secondary_labels=['name', 'responsible_party', 'address', 'city'])\n",
    "\n",
    "    general_inquiries = {}\n",
    "    if soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div', {'class': 'well contact-card'}).text.strip() == '':\n",
    "        t_d['general_contact'] = general_inquiries\n",
    "    else:\n",
    "        general_inquiries['organization'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Name of organization / entity').findNext('div').text.strip()\n",
    "        general_inquiries['responsible_party'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Full name of responsible person').findNext('div').text.strip()\n",
    "        general_inquiries['position'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Position').findNext('div').text.strip()\n",
    "        general_inquiries['other_areas_of_work'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Other areas of specialty/work').findNext('div').text.strip()\n",
    "        general_inquiries['address'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Street address').findNext('div').text.strip()\n",
    "        general_inquiries['city'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='City').findNext('div').text.strip()\n",
    "        general_inquiries['postal_code'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Postal code').findNext('div').text.strip()\n",
    "        general_inquiries['phone'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Phone').findNext('div').text.strip()\n",
    "        if soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Fax'):\n",
    "            general_inquiries['fax'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Fax').findNext('div').text.strip()\n",
    "        else:\n",
    "            general_inquiries['fax'] = None\n",
    "        general_inquiries['email'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Email').findNext('div').text.strip()\n",
    "        if soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Web page address'):\n",
    "            general_inquiries['web_page'] = soup.find('h3', text=re.compile('Person responsible for general inquiries')).findNext('div').find('strong', text='Web page address').findNext('div').text.strip()\n",
    "        else:\n",
    "            general_inquiries['web_page'] = None\n",
    "        t_d['general_contact'] = general_inquiries\n",
    "\n",
    "    scientific_inquiries = {}\n",
    "    if soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div', {'class': 'well contact-card'}).text.strip() == '':\n",
    "        t_d['scientific_contact'] = scientific_inquiries\n",
    "    else:    \n",
    "        scientific_inquiries['organization'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Name of organization / entity').findNext('div').text.strip()\n",
    "        scientific_inquiries['responsible_party'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Full name of responsible person').findNext('div').text.strip()\n",
    "        scientific_inquiries['position'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Position').findNext('div').text.strip()\n",
    "        scientific_inquiries['other_areas_of_work'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Other areas of specialty/work').findNext('div').text.strip()\n",
    "        scientific_inquiries['address'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Street address').findNext('div').text.strip()\n",
    "        scientific_inquiries['city'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='City').findNext('div').text.strip()\n",
    "        scientific_inquiries['postal_code'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Postal code').findNext('div').text.strip()\n",
    "        scientific_inquiries['phone'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Phone').findNext('div').text.strip()\n",
    "        if soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Fax'):\n",
    "            scientific_inquiries['fax'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Fax').findNext('div').text.strip()\n",
    "        else:\n",
    "            scientific_inquiries['fax'] = None\n",
    "        scientific_inquiries['email'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Email').findNext('div').text.strip()\n",
    "        if soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Web page address'):\n",
    "            scientific_inquiries['web_page'] = soup.find('h3', text=re.compile('Person responsible for scientific inquiries')).findNext('div').find('strong', text='Web page address').findNext('div').text.strip()\n",
    "        else:\n",
    "            scientific_inquiries['web_page'] = None\n",
    "        t_d['scientific_contact'] = scientific_inquiries\n",
    "\n",
    "    data_uploading = {}\n",
    "    if soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div', {'class': 'well contact-card'}).text.strip() == '':\n",
    "        t_d['data_update_responsible'] = data_uploading\n",
    "    else:\n",
    "        data_uploading['organization'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Name of organization / entity').findNext('div').text.strip()\n",
    "        data_uploading['responsible_party'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Full name of responsible person').findNext('div').text.strip()\n",
    "        data_uploading['position'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Position').findNext('div').text.strip()\n",
    "        data_uploading['other_areas_of_work'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Other areas of specialty/work').findNext('div').text.strip()\n",
    "        data_uploading['address'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Street address').findNext('div').text.strip()\n",
    "        data_uploading['city'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='City').findNext('div').text.strip()\n",
    "        data_uploading['postal_code'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Postal code').findNext('div').text.strip()\n",
    "        data_uploading['phone'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Phone').findNext('div').text.strip()\n",
    "        if soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Fax'):\n",
    "            data_uploading['fax'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Fax').findNext('div').text.strip()\n",
    "        else:\n",
    "            data_uploading['fax'] = None\n",
    "        data_uploading['email'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Email').findNext('div').text.strip()\n",
    "        if soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Web page address'):\n",
    "            data_uploading['web_page'] = soup.find('h3', text=re.compile('Person responsible for updating data')).findNext('div').find('strong', text='Web page address').findNext('div').text.strip()\n",
    "        else:\n",
    "            data_uploading['web_page'] = None\n",
    "        t_d['data_update_responsible'] = data_uploading\n",
    "\n",
    "    sharing_plan = {}\n",
    "    sharing_plan['ipd_sharing'] = soup.find('h3', text=re.compile('Sharing plan')).findNext('div').find('dt', text=re.compile('Deidentified Individual Participant Data Set \\(IPD\\)')).findNext('dd').text.strip()\n",
    "    sharing_plan['study_protocol'] = soup.find('h3', text=re.compile('Sharing plan')).findNext('div').find('dt', text=re.compile('Study Protocol')).findNext('dd').text.strip()\n",
    "    sharing_plan['statistical_analysis_plan'] = soup.find('h3', text=re.compile('Sharing plan')).findNext('div').find('dt', text=re.compile('Statistical Analysis Plan')).findNext('dd').text.strip()\n",
    "    sharing_plan['informed_consent_form'] = soup.find('h3', text=re.compile('Sharing plan')).findNext('div').find('dt', text=re.compile('Informed Consent Form')).findNext('dd').text.strip()\n",
    "    sharing_plan['clinical_study_report'] = soup.find('h3', text=re.compile('Sharing plan')).findNext('div').find('dt', text=re.compile('Clinical Study Report')).findNext('dd').text.strip()\n",
    "    sharing_plan['analytic_code'] = soup.find('h3', text=re.compile('Sharing plan')).findNext('div').find('dt', text=re.compile('Analytic Code')).findNext('dd').text.strip()\n",
    "    sharing_plan['data_dictionary'] = soup.find('h3', text=re.compile('Sharing plan')).findNext('div').find('dt', text=re.compile('Data Dictionary')).findNext('dd').text.strip()\n",
    "    t_d['sharing_plan'] = sharing_plan\n",
    "    return t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['trial_id', 'trial_name', 'registration_date', 'registration_timing', 'last_updated_date', 'number_of_updates',\n",
    "           'trial_summary', 'registrant_info', 'trial_status', 'funding_source', 'expected_start_date', \n",
    "           'expected_recruitment_end_date', 'actual_start_date', 'actual_recruitment_end_date', 'trial_completion_date', \n",
    "           'scientific_title', 'public_title', 'trial_purpose', 'inclusion_exclusion_criteria', 'subject_age', 'subject_gender', 'phase', 'masking', 'sample_size',\n",
    "           'randomization', 'randomization_description', 'blinding', 'blinding_description', 'placebo', 'assignment', \n",
    "           'other_design_features', 'secondary_ids', 'ethics_information', 'health_conditions', 'primary_outcomes', \n",
    "           'secondary_outcomes', 'intervention_groups', 'recruitement_centers', 'sponsor_funding_sources', 'general_contact', \n",
    "           'scientific_contact', 'data_update_responsible', 'sharing_plan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are currently 22,217 registered trials in the IRCT as of 28 Oct 2019. The trials are numbered sequentially,\n",
    "#But there is no good way to get the max trial number. It appears that the highest trials are around 40,000 so we will use\n",
    "#50000 as a safe number. Will check to make sure this tracks\n",
    "\n",
    "trial_ids = list(range(0,50000))\n",
    "base_irct_url = 'https://www.irct.ir/trial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bff93cb517422a92e1962145b4940e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scrape Finished in 4 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "with open('irct_trials.csv', 'w', newline='', encoding='utf-8') as irct_csv:\n",
    "    writer = csv.DictWriter(irct_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    request = 0\n",
    "    for i in tqdm(trial_ids[0:500]):\n",
    "        soup = get_url(base_irct_url + str(i))\n",
    "        if soup.find('div', {'class':'message'}, text=re.compile('Not Found')):\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                trial_info = get_row(soup)\n",
    "                writer.writerow(trial_info)\n",
    "            except Exception as e:\n",
    "                import sys\n",
    "                raise type(e)(str(e) + '\\n' + 'Error trial: {}'.format(i)).with_traceback(sys.exc_info()[2])\n",
    "end_time = time()\n",
    "print('Scrape Finished in {} minutes'.format(round((end_time-start_time) / 60),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check this with smaller sample"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
