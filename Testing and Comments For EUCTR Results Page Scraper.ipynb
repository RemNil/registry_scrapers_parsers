{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful references for building this scraper\n",
    "#https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe\n",
    "#https://www.dataquest.io/blog/web-scraping-beautifulsoup/\n",
    "\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will need the base EUCTR URL throughout the project\n",
    "euctr_base_url = 'https://www.clinicaltrialsregister.eu'\n",
    "\n",
    "#For testing, we pull page 1 of the advanced search URL that produces only trials that have results\n",
    "\n",
    "url = 'https://www.clinicaltrialsregister.eu/ctr-search/search?query=&resultsstatus=trials-with-results&page1'\n",
    "response = get(url, verify = False)\n",
    "html = response.content\n",
    "\n",
    "#what does our parsed html look like?\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each trial appears in the search as a series of tables. There are 20 on a full page of search results.\n",
    "#This extracts all the trial tables from the larger HTML\n",
    "tables = soup.find_all('table', {'class': 'result'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick check to make sure everything looks correct\n",
    "print(type(tables))\n",
    "print(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at just to tables HTML so we can start extracting what we need\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment with just the first search result\n",
    "first_trial = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts the EudraCT number from the first trial, then prints to test that it extracted correctly\n",
    "first_trial_id = first_trial.input.get('value')\n",
    "print(len(first_trial_id))\n",
    "print(first_trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts the part of the URL that leads to the results URL and appends it to the base URL\n",
    "first_results_link = euctr_base_url + first_trial.find_all('a')[-1].get('href')\n",
    "print(first_results_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the next trial for testing to make sure above code still works\n",
    "second_trial = tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_trial_id = second_trial.input.get('value')\n",
    "print(len(second_trial_id))\n",
    "print(second_trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_results_link = euctr_base_url + second_trial.find_all('a')[-1].get('href')\n",
    "print(second_results_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blank lists for collectiong all trial ids and results urls from the first page of results. That's all we need for now.\n",
    "trial_ids_first_page = []\n",
    "results_urls_first_page = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the above testing for getting tht data from single trials and generalizes it to a loop and then makes sure it worked\n",
    "for table in tables:\n",
    "    trial_id = table.input.get('value')\n",
    "    trial_ids_first_page.append(trial_id)\n",
    "    url = euctr_base_url + table.find_all('a')[-1].get('href')\n",
    "    results_urls_first_page.append(url)\n",
    "print(trial_ids_first_page)\n",
    "print(results_urls_first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For when this scrape us run on the full EUCTR, we will need to know how many pages long the \"has results\" search result is.\n",
    "#This is how we extract that information. For lack of a better method, this uses a regular expression.\n",
    "number_of_pages = soup.find('div', {'class': 'margin-bottom: 6px;'})\n",
    "max_page_link = str(number_of_pages.find_all('a')[-1])\n",
    "max_page = re.findall(r'\\d+', max_page_link)[0]\n",
    "print(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the method from the link at the beginning of this notebook, \n",
    "#we use the testing above to create a test crawler that will run on the first 5 pages of search results\n",
    "pages = [str(i) for i in range(1,6)]\n",
    "print(pages)\n",
    "print('https://www.clinicaltrialsregister.eu/ctr-search/search?query=&resultsstatus=trials-with-results&page=' + pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids = []\n",
    "results_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "#for each of the first 5 pages of results\n",
    "for page in pages:\n",
    "    \n",
    "    #make this request\n",
    "    response = get('https://www.clinicaltrialsregister.eu/ctr-search/search?query=&resultsstatus=trials-with-results&page=' + page, verify = False)\n",
    "    \n",
    "    #pause to look like a human\n",
    "    sleep(randint(1,4)) #this can likely be reduced quite a bit. Perhaps to just 1,2 or even just 1\n",
    "    \n",
    "    #mointor the requests to ensure everything is working\n",
    "    requests += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Throw a warning for a non-200 status code\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "    #Break the looop if we exceed the number of requests which will need to change when i do full scrape\n",
    "    if requests > 5:\n",
    "        warn('Number of requests was greater than expected.')  \n",
    "        break \n",
    "    \n",
    "    #Parse the requests\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #select all the trial tables\n",
    "    trial_tables = page_html.find_all('table', {'class': 'result'})\n",
    "    \n",
    "    #get the trial id and the trial url for each thing\n",
    "    for trial_table in trial_tables:\n",
    "        trial_id = trial_table.input.get('value')\n",
    "        trial_ids.append(trial_id)\n",
    "        url = euctr_base_url + trial_table.find_all('a')[-1].get('href')\n",
    "        results_urls.append(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It works! We now can extract all the trial IDs that appear in the search results for a results only search.\n",
    "print(trial_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the next step, we first need to create the components of the actual results URL that we can stick the above trial_ids into\n",
    "results_base_url = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/'\n",
    "results_end_url = '/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So far, I have only ever come across 2 ways an EUCTR results page can look. It will either be a page with tabular results\n",
    "#or a page with a synopsis. They each have slightly different data so we need to do some testing with both.\n",
    "#Examples of each type below\n",
    "\n",
    "#tabular\n",
    "results_test_url_1 = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/2015-001216-35/results'\n",
    "\n",
    "#synopsis\n",
    "results_test_url_2 = 'https://www.clinicaltrialsregister.eu/ctr-search/trial/2004-000086-35/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting and parsing the tabular example\n",
    "\n",
    "results_1 = get(results_test_url_1, verify = False)\n",
    "results_1_html = results_1.content\n",
    "\n",
    "soup2 = BeautifulSoup(results_1_html, \"html.parser\")\n",
    "print(soup2.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It appears that the table containing the information we want on the trial results is always the 5th one down on a page (zero indexed)\n",
    "\n",
    "results_table_1 = soup2.find_all('table')[4]\n",
    "print(results_table_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how we get the trial id\n",
    "r1_trial_id = results_table_1.find_all('a')[0].get_text()\n",
    "print(r1_trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a sub-extraction of the section that contains everything else we need\n",
    "r1_tds = results_table_1.find_all('td', class_ = 'valueColumn')\n",
    "print(r1_tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function that gets the piece of data from the part of the table we want, and cleans it up\n",
    "def tds_strip(td_table, td):\n",
    "    return td_table[td].div.get_text().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global end of trial date\n",
    "print(tds_strip(r1_tds,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First version publication date\n",
    "print(tds_strip(r1_tds,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This version publication date\n",
    "print(tds_strip(r1_tds,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current version\n",
    "rd1_version = r1_tds[5].get_text().strip()\n",
    "print(rd1_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can move on to the synopsis results page for testing. First we parse.\n",
    "\n",
    "results_2 = get(results_test_url_2, verify = False)\n",
    "results_2_html = results_2.content\n",
    "\n",
    "soup3 = BeautifulSoup(results_2_html, \"html.parser\")\n",
    "print(soup3.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once again, we want the 5th table on the page\n",
    "results_table_2 = soup3.find_all('table')[4]\n",
    "print(results_table_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the trial id\n",
    "r2_trial_id = results_table_2.find_all('a')[0].get_text()\n",
    "print(r2_trial_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the sub-extraction for the test of the data \n",
    "#and then using our function from before to extract and clean up at the locations for the data we want\n",
    "r2_tds = results_table_2.find_all('td', class_ = 'valueColumn')\n",
    "print(r2_tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global end of trial date\n",
    "print(tds_strip(r2_tds,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This version publication date\n",
    "print(tds_strip(r2_tds,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First version publication date\n",
    "print(tds_strip(r2_tds,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current version\n",
    "rd2_version = r2_tds[9].get_text().strip()\n",
    "print(rd2_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One extra thing to extract here is the presense of the link to synopsis. This is what will allow us to differentiate \n",
    "#what type of page we are scraping when we run the full crawler so we know which information indexes to use\n",
    "r2_tds_lc = results_table_2.find_all('td', class_ = 'labelColumn')\n",
    "print(r2_tds_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will turn into our results type indicator\n",
    "rd2_attachment = r2_tds_lc[-1].div.get_text().strip()\n",
    "print(rd2_attachment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the lists for our data\n",
    "results_trial_id = []\n",
    "global_end_of_trial_date = []\n",
    "first_publication_date = []\n",
    "current_publication_date = []\n",
    "results_version = []\n",
    "results_type = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a quick test on 11 trial ids to run in the crawler below\n",
    "test_results = trial_ids[0:11]\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_2 = time()\n",
    "requests_2 = 0\n",
    "\n",
    "#for each of the first 5 pages of results\n",
    "for test_result in test_results:\n",
    "    \n",
    "    #make this request\n",
    "    response = get(results_base_url + test_result + results_end_url, verify = False)\n",
    "    \n",
    "    #pause to look like a human\n",
    "    sleep(randint(1,4))\n",
    "    \n",
    "    #mointor the requests to ensure everything is working\n",
    "    requests_2 += 1\n",
    "    elapsed_time = time() - start_time_2\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(requests_2, requests_2/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Throw a warning for a non-200 status code\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests_2, response.status_code))\n",
    "\n",
    "    #Break the looop if we exceed the number of requests which will need to change when i do full scrape\n",
    "    if requests_2 > 100:\n",
    "        warn('Number of requests was greater than expected.')  \n",
    "        break \n",
    "    \n",
    "    #Parse the requests\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #select all the results tables\n",
    "    trial_tables = page_html.find_all('table')[4]\n",
    "    td_value = trial_tables.find_all('td', class_ = 'valueColumn')\n",
    "    td_label = trial_tables.find_all('td', class_ = 'labelColumn') \n",
    "    \n",
    "    if td_label[-1].div.get_text().strip() == 'Summary report(s)':\n",
    "        trial_id = trial_tables.find_all('a')[0].get_text()\n",
    "        results_trial_id.append(trial_id)\n",
    "        global_end_date = tds_strip(td_value,3)\n",
    "        global_end_of_trial_date.append(global_end_date)\n",
    "        first_pub = tds_strip(td_value,11)\n",
    "        first_publication_date.append(first_pub)\n",
    "        current_pub = tds_strip(td_value,10)\n",
    "        current_publication_date.append(current_pub)\n",
    "        version = td_value[9].get_text().strip()\n",
    "        results_version.append(version)\n",
    "        results_type.append(\"Document\")\n",
    "        \n",
    "    else:\n",
    "        trial_id = trial_tables.find_all('a')[0].get_text()\n",
    "        results_trial_id.append(trial_id)\n",
    "        global_end_date = tds_strip(td_value,3)\n",
    "        global_end_of_trial_date.append(global_end_date)\n",
    "        first_pub = tds_strip(td_value,7)\n",
    "        first_publication_date.append(first_pub)\n",
    "        current_pub = tds_strip(td_value,6)\n",
    "        current_publication_date.append(current_pub)\n",
    "        version = td_value[5].get_text().strip()\n",
    "        results_version.append(version)\n",
    "        results_type.append(\"Tabular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets print a few of our lists just to check that it looked ok\n",
    "print(results_trial_id)\n",
    "print(global_end_of_trial_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets make a dataframe to check how everything turned out\n",
    "\n",
    "test_df = pd.DataFrame({'trial_id': results_trial_id,\n",
    "                       'global_trial_end_date': global_end_of_trial_date,\n",
    "                       'first_pub_date': first_publication_date,\n",
    "                       'current_pub_date': current_publication_date,\n",
    "                       'version': results_version,\n",
    "                       'results_type': results_type})\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next things to do\n",
    "# 1. Get the dates to act like dates\n",
    "# 2. Make a much more condensed version of the crawlers without all the testing\n",
    "# 3. Run them in full and make sure they work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
