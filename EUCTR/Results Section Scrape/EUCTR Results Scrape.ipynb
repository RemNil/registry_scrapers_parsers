{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "from IPython.core.display import clear_output\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndevito\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "#pull single page to test and get max page number\n",
    "url = 'https://www.clinicaltrialsregister.eu/ctr-search/search?query=&resultsstatus=trials-with-results&page1'\n",
    "response = get(url, verify = False)\n",
    "html = response.content\n",
    "\n",
    "#what does our parsed html look like?\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n"
     ]
    }
   ],
   "source": [
    "#gets max page number\n",
    "number_of_pages = soup.find('div', {'class': 'margin-bottom: 6px;'})\n",
    "max_page_link = str(number_of_pages.find_all('a')[-1])\n",
    "max_page = re.findall(r'\\d+', max_page_link)[0]\n",
    "print(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up variables for page URLs\n",
    "euctr_base_url = 'https://www.clinicaltrialsregister.eu'\n",
    "euctr_results_search_page = '/ctr-search/search?query=&resultsstatus=trials-with-results&page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n"
     ]
    }
   ],
   "source": [
    "#variables needed for first scrape\n",
    "pages = [str(i) for i in range(1,int(max_page)+1)]\n",
    "trial_ids = []\n",
    "results_urls = []\n",
    "start_time = time()\n",
    "requests = 0\n",
    "print(pages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request: 605; Frequency: 3.3889122904725797 requests/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndevito\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "#this crawls every trial that comes up with a search result of trials with results on the EUCTR\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #make this request\n",
    "    tries=3\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            response = get(euctr_base_url + euctr_results_search_page + page, verify = False)\n",
    "            break\n",
    "        except ConnectionError as e:\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    #pause to look like a human\n",
    "    #sleep(random.uniform(0,0.2)) #not needed at the moment for this\n",
    "    \n",
    "    #mointor the requests to ensure everything is working\n",
    "    requests += 1\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Throw a warning for a non-200 status code\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "    #Break the looop if we exceed the number of requests which will need to change when i do full scrape\n",
    "    if requests > int(max_page):\n",
    "        warn('Number of requests was greater than expected.')  \n",
    "        break \n",
    "    \n",
    "    #Parse the requests\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #select all the trial tables\n",
    "    trial_tables = page_html.find_all('table', {'class': 'result'})\n",
    "    \n",
    "    #get the trial id and the trial url for each thing\n",
    "    for trial_table in trial_tables:\n",
    "        trial_id = trial_table.input.get('value')\n",
    "        trial_ids.append(trial_id)\n",
    "        url = euctr_base_url + trial_table.find_all('a')[-1].get('href')\n",
    "        results_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_trial_id = []\n",
    "global_end_of_trial_date = []\n",
    "first_publication_date = []\n",
    "current_publication_date = []\n",
    "results_version = []\n",
    "results_type = []\n",
    "start_time_2 = time()\n",
    "requests_2 = 0\n",
    "trial_number = 0\n",
    "\n",
    "def tds_strip(td_table, td):\n",
    "    return td_table[td].div.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndevito\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Number: 12087; Request: 12087; Frequency: 1.1794330580403458 requests/s\n"
     ]
    }
   ],
   "source": [
    "#this takes the urls from the above scrape, and crawls them for results information\n",
    "\n",
    "for result_url in results_urls: \n",
    "    \n",
    "    #make this request\n",
    "    tries=3\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            requests_2 += 1\n",
    "            response = get(result_url, verify = False)\n",
    "            break\n",
    "        except ConnectionError as e:\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                raise     \n",
    "    \n",
    "    #pause to look like a human\n",
    "    #sleep(random.uniform(0,0.5)) #uncomment if needed\n",
    "    \n",
    "    #mointor the requests to ensure everything is working\n",
    "    trial_number += 1\n",
    "    elapsed_time = time() - start_time_2\n",
    "    print('Trial Number: {}; Request: {}; Frequency: {} requests/s'.format(trial_number, requests_2, requests_2/elapsed_time))\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Throw a warning for a non-200 status code\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests_2, response.status_code))\n",
    "\n",
    "    #Break the looop if we exceed the number of requests which will need to change when i do full scrape\n",
    "    if requests_2 > len(results_urls):\n",
    "        warn('Number of requests was greater than expected.')  \n",
    "        break \n",
    "    \n",
    "    #Parse the requests\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #select all the results tables\n",
    "    leg_text = page_html.find('div', id = 'synopsisLegislationNote')\n",
    "    trial_tables = page_html.find_all('table')[4]\n",
    "    td_value = trial_tables.find_all('td', class_ = 'valueColumn')\n",
    "    td_label = trial_tables.find_all('td', class_ = 'labelColumn') \n",
    "    \n",
    "    if td_label[-1].div.get_text().strip() == 'Summary report(s)' and leg_text is not None:\n",
    "        trial_id = trial_tables.find_all('a')[0].get_text()\n",
    "        results_trial_id.append(trial_id)\n",
    "        global_end_date = tds_strip(td_value,3)\n",
    "        global_end_of_trial_date.append(global_end_date)\n",
    "        first_pub = tds_strip(td_value,11)\n",
    "        first_publication_date.append(first_pub)\n",
    "        current_pub = tds_strip(td_value,10)\n",
    "        current_publication_date.append(current_pub)\n",
    "        version = td_value[9].get_text().strip()\n",
    "        results_version.append(version)\n",
    "        results_type.append(\"Document\")\n",
    "    \n",
    "    elif td_label[-1].div.get_text().strip() == 'Summary report(s)' and leg_text is None:\n",
    "        trial_id = trial_tables.find_all('a')[0].get_text()\n",
    "        results_trial_id.append(trial_id)\n",
    "        global_end_date = tds_strip(td_value,3)\n",
    "        global_end_of_trial_date.append(global_end_date)\n",
    "        first_pub = tds_strip(td_value,7)\n",
    "        first_publication_date.append(first_pub)\n",
    "        current_pub = tds_strip(td_value,6)\n",
    "        current_publication_date.append(current_pub)\n",
    "        version = td_value[5].get_text().strip()\n",
    "        results_version.append(version)\n",
    "        results_type.append(\"Mixed\")\n",
    "        \n",
    "    else:\n",
    "        trial_id = trial_tables.find_all('a')[0].get_text()\n",
    "        results_trial_id.append(trial_id)\n",
    "        global_end_date = tds_strip(td_value,3)\n",
    "        global_end_of_trial_date.append(global_end_date)\n",
    "        first_pub = tds_strip(td_value,7)\n",
    "        first_publication_date.append(first_pub)\n",
    "        current_pub = tds_strip(td_value,6)\n",
    "        current_publication_date.append(current_pub)\n",
    "        version = td_value[5].get_text().strip()\n",
    "        results_version.append(version)\n",
    "        results_type.append(\"Tabular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Scraped\n"
     ]
    }
   ],
   "source": [
    "if len(trial_ids) == len(results_trial_id):\n",
    "    print(\"All Scraped\")\n",
    "else:\n",
    "    print(\"Error in Scrape: Difference of {} trials between first and second scrape\".format(len(trial_ids) - len(results_trial_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_id</th>\n",
       "      <th>global_trial_end_date</th>\n",
       "      <th>first_pub_date</th>\n",
       "      <th>current_pub_date</th>\n",
       "      <th>version</th>\n",
       "      <th>results_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12082</th>\n",
       "      <td>2013-000706-36</td>\n",
       "      <td>06 Jan 2017</td>\n",
       "      <td>30 Dec 2017</td>\n",
       "      <td>30 Dec 2017</td>\n",
       "      <td>v1(current)</td>\n",
       "      <td>Tabular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12083</th>\n",
       "      <td>2010-019821-32</td>\n",
       "      <td>16 Dec 2016</td>\n",
       "      <td>23 Dec 2017</td>\n",
       "      <td>23 Dec 2017</td>\n",
       "      <td>v1(current)</td>\n",
       "      <td>Tabular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>2013-000568-28</td>\n",
       "      <td>13 Mar 2017</td>\n",
       "      <td>22 Mar 2018</td>\n",
       "      <td>22 Mar 2018</td>\n",
       "      <td>v1(current)</td>\n",
       "      <td>Tabular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12085</th>\n",
       "      <td>2012-003242-33</td>\n",
       "      <td>08 Feb 2017</td>\n",
       "      <td>16 Feb 2018</td>\n",
       "      <td>16 Feb 2018</td>\n",
       "      <td>v1(current)</td>\n",
       "      <td>Tabular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12086</th>\n",
       "      <td>2014-001487-35</td>\n",
       "      <td>05 Jan 2017</td>\n",
       "      <td>07 Jan 2018</td>\n",
       "      <td>17 Jun 2018</td>\n",
       "      <td>v2(current)</td>\n",
       "      <td>Tabular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trial_id global_trial_end_date first_pub_date current_pub_date  \\\n",
       "12082  2013-000706-36           06 Jan 2017    30 Dec 2017      30 Dec 2017   \n",
       "12083  2010-019821-32           16 Dec 2016    23 Dec 2017      23 Dec 2017   \n",
       "12084  2013-000568-28           13 Mar 2017    22 Mar 2018      22 Mar 2018   \n",
       "12085  2012-003242-33           08 Feb 2017    16 Feb 2018      16 Feb 2018   \n",
       "12086  2014-001487-35           05 Jan 2017    07 Jan 2018      17 Jun 2018   \n",
       "\n",
       "           version results_type  \n",
       "12082  v1(current)      Tabular  \n",
       "12083  v1(current)      Tabular  \n",
       "12084  v1(current)      Tabular  \n",
       "12085  v1(current)      Tabular  \n",
       "12086  v2(current)      Tabular  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'trial_id': results_trial_id,\n",
    "                       'global_trial_end_date': global_end_of_trial_date,\n",
    "                       'first_pub_date': first_publication_date,\n",
    "                       'current_pub_date': current_publication_date,\n",
    "                       'version': results_version,\n",
    "                       'results_type': results_type\n",
    "                          })\n",
    "\n",
    "results_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned 8 Rows\n"
     ]
    }
   ],
   "source": [
    "#some data cleaning\n",
    "\n",
    "cleaned = 0\n",
    "for index, row in results_df.iterrows():\n",
    "    if row.first_pub_date == 'No':\n",
    "        results_df.at[index, 'first_pub_date'] = None\n",
    "        results_df.at[index, 'current_pub_date'] = None\n",
    "        results_df.at[index, 'version'] = None\n",
    "        results_df.at[index, 'results_type'] = 'None Available'\n",
    "        cleaned += 1\n",
    "\n",
    "#dates to datetime\n",
    "results_df['global_trial_end_date'] = pd.to_datetime(results_df['global_trial_end_date'])\n",
    "results_df['first_pub_date'] = pd.to_datetime(results_df['first_pub_date'])\n",
    "results_df['current_pub_date'] = pd.to_datetime(results_df['current_pub_date'])\n",
    "print(\"Cleaned {} Rows\".format(cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial_id                         object\n",
       "global_trial_end_date    datetime64[ns]\n",
       "first_pub_date           datetime64[ns]\n",
       "current_pub_date         datetime64[ns]\n",
       "version                          object\n",
       "results_type                     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a csv\n",
    "results_df.to_csv('euctr_results_scrape_may2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
