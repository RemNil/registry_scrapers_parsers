{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_urls = 'https://slctr.lk/trials/search?page={}&s=&type=by_pt'\n",
    "\n",
    "soup = get_url('https://slctr.lk/trials/search?page=1&s=&type=by_pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page = soup.find('div', {'class':'pagination'}).find_all('li')[-2].text\n",
    "\n",
    "page_list = list(range(1,int(max_page)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351252b9a0a446c29b255279d5873221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=35.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "id_list = []\n",
    "for page in tqdm(page_list):\n",
    "    soup = get_url(search_urls.format(page))\n",
    "    td_list = soup.find('table', {'class', 'table table-striped'}).find_all('td', text=re.compile('^SLCTR'))\n",
    "    for td in td_list:\n",
    "        tid = td.text\n",
    "        id_list.append(tid.replace('/','-'))\n",
    "\n",
    "base_trial_urls = 'https://slctr.lk/trials/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_field(soup, field_name):\n",
    "    return soup.find('p', text=field_name).findNext('p').text.strip()\n",
    "\n",
    "def get_row(soup):\n",
    "    t_d = {}\n",
    "    t_d['trial_id'] = normal_field(soup, 'SLCTR Registration Number')\n",
    "    t_d['trial_title'] = soup.find('h2', {'class': 'trial-title'}).text.replace('\\n- ','')\n",
    "    t_d['registration_date'] = soup.find('p', text='Date of Registration').findNext('div').text.strip()\n",
    "    t_d['last_updated_date'] = soup.find('p', text='The date of last modification').findNext('div').text.strip()\n",
    "    t_d['scientific_title'] = soup.find('p', text='Scientific Title of Trial').findNext('p').findNext('p').text\n",
    "\n",
    "    t_d['public_title'] = normal_field(soup, 'Public Title of Trial')\n",
    "    t_d['brief_title'] = normal_field(soup, 'Brief title')\n",
    "    t_d['study_disease_condition'] = normal_field(soup, 'Disease or Health Condition(s) Studied')\n",
    "\n",
    "    t_d['scientific_acronym'] = normal_field(soup, 'Scientific Acronym')\n",
    "    t_d['public_acronym'] = normal_field(soup, 'Public Acronym')\n",
    "    t_d['universal_trial_number'] = normal_field(soup, 'Universal Trial Number')\n",
    "    t_d['other_trial_ids'] = normal_field(soup,'Any other number(s) assigned to the trial and issuing authority')\n",
    "    t_d['research_question'] = normal_field(soup,'What is the research question being addressed?')\n",
    "    t_d['study_type'] = normal_field(soup,'Type of study')\n",
    "    t_d['allocation'] = normal_field(soup,'Allocation')\n",
    "    masking = normal_field(soup,'Masking')\n",
    "    t_d['masking'] = \" \".join(masking.split())\n",
    "    t_d['control'] = normal_field(soup,'Control')\n",
    "    t_d['assignment'] = normal_field(soup,'Assignment')\n",
    "    t_d['purpose'] = normal_field(soup,'Purpose')\n",
    "    t_d['phase'] = normal_field(soup,'Study Phase')\n",
    "    t_d['planned_intervention'] = normal_field(soup,'Intervention(s) planned')\n",
    "    t_d['inclusion_criteria'] = soup.find('p', text='Inclusion criteria').findNext('div').text.strip().replace('•','').replace(' – ','-')\n",
    "    t_d['exclusion_criteria'] = soup.find('p', text='Exclusion criteria').findNext('div').text.strip().replace('•','').replace(' – ','-')\n",
    "\n",
    "    outcomes = soup.find('p', text='Primary outcome(s)').findNext('div').table.find_all('div', {'class':'outcome-block'})\n",
    "    timeframes = soup.find('p', text='Primary outcome(s)').findNext('div').table.find_all('em', {'class':'custom-inline'})\n",
    "    pat = re.compile(r'^\\d{1,2}.?\\s$')\n",
    "\n",
    "    outcome_list = []\n",
    "    for o, t in zip(outcomes, timeframes):\n",
    "        outcome_dict = {}\n",
    "        if not pat.match(o.text) and t.text != '[]':\n",
    "            outcome_dict['outcome'] = o.text.strip().replace('\\n','')\n",
    "            outcome_dict['timeframe'] = t.text.strip().replace('\\n','').replace('[','').replace(']','')\n",
    "            outcome_list.append(outcome_dict)\n",
    "        elif pat.match(o.text) and t.text != '[]':\n",
    "            outcome_dict['outcome'] = None\n",
    "            outcome_dict['timeframe'] = t.text.strip().replace('\\n','').replace('[','').replace(']','')\n",
    "            outcome_list.append(outcome_dict)\n",
    "        elif not pat.match(o.text) and t.text == '[]': \n",
    "            outcome_dict['outcome'] = o.text.strip().replace('\\n','')\n",
    "            outcome_dict['timeframe'] = None\n",
    "            outcome_list.append(outcome_dict)\n",
    "        else:\n",
    "            pass\n",
    "    t_d['primary_outcomes'] = outcome_list\n",
    "\n",
    "    s_outcomes = soup.find('p', text='Secondary outcome(s)').findNext('div').table.find_all('div', {'class':'outcome-block'})\n",
    "    s_timeframes = soup.find('p', text='Secondary outcome(s)').findNext('div').table.find_all('em', {'class':'custom-inline'})\n",
    "\n",
    "    s_outcome_list = []\n",
    "    for s, t in zip(s_outcomes, s_timeframes):\n",
    "        outcome_dict = {}\n",
    "        if not pat.match(s.text) and t.text != '[]':\n",
    "            outcome_dict['outcome'] = s.text.strip().replace('\\n','')\n",
    "            outcome_dict['timeframe'] = t.text.strip().replace('\\n','').replace('[','').replace(']','')\n",
    "            outcome_list.append(outcome_dict)\n",
    "        elif pat.match(s.text) and t.text != '[]':\n",
    "            outcome_dict['outcome'] = None\n",
    "            outcome_dict['timeframe'] = t.text.strip().replace('\\n','').replace('[','').replace(']','')\n",
    "            outcome_list.append(outcome_dict)\n",
    "        elif not pat.match(s.text) and t.text == '[]': \n",
    "            outcome_dict['outcome'] = s.text.strip().replace('\\n','')\n",
    "            outcome_dict['timeframe'] = None\n",
    "            outcome_list.append(outcome_dict)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    t_d['secondary_outcomes'] = s_outcome_list\n",
    "\n",
    "    t_d['target_enrollment'] = normal_field(soup,'Target number/sample size')\n",
    "    countries = normal_field(soup,'Countries of recruitment')\n",
    "    if ',' in countries:\n",
    "        t_d['recruitment_countries'] = countries.split(', ')\n",
    "    else:\n",
    "        t_d['recruitment_countries'] = normal_field(soup,'Countries of recruitment')\n",
    "    t_d['anticipated_start_date'] = normal_field(soup,'Anticipated start date')\n",
    "    t_d['anticipated_completion_date'] = normal_field(soup,'Anticipated end date')\n",
    "    t_d['first_enrollment_date'] = normal_field(soup,'Date of first enrollment')\n",
    "    t_d['study_completion_date'] = normal_field(soup,'Date of study completion')\n",
    "    t_d['trial_status'] = normal_field(soup,'Recruitment status')\n",
    "    t_d['funcing_source'] = normal_field(soup,'Funding source')\n",
    "    t_d['regulatory_approvals'] = normal_field(soup,'Regulatory approvals')\n",
    "\n",
    "    ethics_information = {}\n",
    "    ethics_information['status'] = normal_field(soup,'Status')\n",
    "    ethics_information['approval_date'] = normal_field(soup,'Date of Approval')\n",
    "    ethics_information['approval_number'] = normal_field(soup,'Approval number')\n",
    "    ethics_information['committee'] = soup.find('span', text='Name:').findNext('span').text.strip()\n",
    "    ethics_information['address'] = soup.find('span', text='Institutional Address:').findNext('span').text.strip().replace('\\r','').replace('\\n',' ')\n",
    "    ethics_information['phone'] = soup.find('span', text='Telephone:').findNext('span').text.strip()\n",
    "    ethics_information['email'] = soup.find('span', text='Email:').findNext('span').text.strip()\n",
    "\n",
    "    t_d['ethics_information'] = ethics_information\n",
    "    t_d['scientific_contact'] = normal_field(soup,'Contact person for Scientific Queries/Principal Investigator').replace('\\r','').replace('\\n',' ')\n",
    "    t_d['public_contact'] = normal_field(soup,'Contact Person for Public Queries').replace('\\r','').replace('\\n',' ')\n",
    "    t_d['primary_sponsor'] = normal_field(soup,'Primary study sponsor/organization').replace('\\r','').replace('\\n',' ')\n",
    "    t_d['secondary_sponsor'] = normal_field(soup,'Secondary study sponsor (If any)').replace('\\r','').replace('\\n',' ')\n",
    "\n",
    "    t_d['ipd_sharing'] = normal_field(soup,'Do the investigators plan to share identified individual clinical trial participant-level data (IPD)?')\n",
    "    t_d['ipd_sharing_plan'] = normal_field(soup, 'IPD sharing plan description')\n",
    "    t_d['protocol_available']  = normal_field(soup, 'Study protocol available')\n",
    "    t_d['protocol_version_date']  = normal_field(soup, 'Protocol version and date')\n",
    "    t_d['protocol_url']  = soup.find('p', text='Protocol URL').findNext('div').text.strip()\n",
    "    t_d['summary_results_available']  = normal_field(soup, 'Results summary available')\n",
    "    t_d['results_posting_date']  = normal_field(soup, 'Date of posting results')\n",
    "    t_d['final_completion_date'] = normal_field(soup, 'Date of study completion')\n",
    "    t_d['final_enrollment']  = normal_field(soup, 'Final sample size')\n",
    "    t_d['date_of_first_publication']  = normal_field(soup, 'Date of first publication')\n",
    "    t_d['link_to_results'] = soup.find('p', text='Link to results').findNext('div').text.strip()\n",
    "    t_d['results_summary']  = normal_field(soup, 'Brief summary of results')\n",
    "    return t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_check(soup):\n",
    "    if soup.find('table', {'class':'table table-striped'}):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d08058aa4547d386263e614659d0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=342.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trial_data = []\n",
    "\n",
    "for i in tqdm(id_list):\n",
    "    soup = get_url(base_trial_urls + i)\n",
    "    trial_info = get_row(soup)\n",
    "    pubs = get_url(base_trial_urls + i + '/publications')\n",
    "    trial_info['pubs_in_tab'] = table_check(pubs)\n",
    "    prog = get_url(base_trial_urls + i + '/progresses')\n",
    "    trial_info['prog_in_tab'] = table_check(prog)\n",
    "    trial_data.append(trial_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['trial_id', 'trial_title', 'registration_date', 'last_updated_date', 'scientific_title', 'public_title', \n",
    "           'brief_title', 'study_disease_condition', 'scientific_acronym', 'public_acronym', 'universal_trial_number', \n",
    "           'other_trial_ids', 'research_question', 'study_type', 'allocation', 'masking', 'control', 'assignment', 'purpose', \n",
    "           'phase', 'planned_intervention', 'inclusion_criteria', 'exclusion_criteria', 'primary_outcomes', 'secondary_outcomes',\n",
    "           'target_enrollment', 'recruitment_countries', 'anticipated_start_date', 'anticipated_completion_date', \n",
    "           'first_enrollment_date', 'study_completion_date', 'trial_status', 'funcing_source', 'regulatory_approvals',\n",
    "           'ethics_information', 'scientific_contact', 'public_contact', 'primary_sponsor', 'secondary_sponsor',\n",
    "           'ipd_sharing', 'ipd_sharing_plan', 'protocol_available', 'protocol_version_date', 'protocol_url',\n",
    "           'summary_results_available', 'results_posting_date', 'final_completion_date', 'final_enrollment', \n",
    "           'date_of_first_publication', 'link_to_results', 'results_summary', 'pubs_in_tab', 'prog_in_tab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "with open('slctr_trials_{}.csv'.format(date.today()), 'w', newline='', encoding='utf-8') as slctr_csv:\n",
    "    writer = csv.DictWriter(slctr_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for td in trial_data:\n",
    "        writer.writerow(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
