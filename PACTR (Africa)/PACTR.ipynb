{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "def get_url(url):\n",
    "    response = get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_description_labels = ['public_title', 'scientific_title', 'background', 'study_type', 'acronym', 'primary_conditions', \n",
    "                     'secondary_conditions', 'purpose', 'anticipated_start_date', 'actual_start_date', \n",
    "                     'anticipated_completion_date', 'actual_completion_date', 'anticiapted_enrollment', 'actual_enrollment', \n",
    "                     'recruitment_status', 'publication_url']\n",
    "\n",
    "design_labels = ['intervention_assignment', 'allocation_process', 'randomisation_generation', 'allocation_concealment', \n",
    "                 'masking', 'masking_blinding_used']\n",
    "\n",
    "int_labels = ['intervention_type', 'intervention_name', 'dose', 'duration', 'intervention_description', 'group_size',\n",
    "                      'nature_of_control']\n",
    "\n",
    "elig_labels = ['inclusion_criteria', 'exclusion_criteria', 'age_category', 'min_age', 'max_age', 'gender']\n",
    "\n",
    "ethics_labels = ['recieved_ethics', 'date_to_be_submitted_approval', 'date_of_approval', 'ethics_committee', \n",
    "                 'committee_address']\n",
    "\n",
    "ethics_address = ['address', 'city', 'postal_code', 'country']\n",
    "\n",
    "outcome_labels = ['outcome_type', 'outcome', 'measurement_timepoints']\n",
    "\n",
    "recruit_labels = ['centre_name', 'address', 'city', 'postal_code', 'country']\n",
    "\n",
    "funding_labels = ['name_of_funder', 'address', 'city', 'postal_code' 'country']\n",
    "\n",
    "spon_labels = ['sponsor_type', 'sponsor_name', 'address', 'city', 'postal_code', 'country', 'nature_of_sponsor']\n",
    "\n",
    "collab_labels = ['name', 'address', 'city', 'postal_code', 'country']\n",
    "\n",
    "contact_labels_1 = ['role', 'name', 'email', 'phone', 'address']\n",
    "\n",
    "contact_labels_2 = ['city', 'postal_code', 'country', 'position_affiliation']\n",
    "\n",
    "results_labels = ['share_ipd', 'description', 'additiona_documents', 'sharing_time_frame', 'key_access_criteria', 'url',\n",
    "                 'results_available', 'results_summary', 'results_posting_date', 'first_journal_pub_date', 'results_url',\n",
    "                 'baseline_characteristics', 'participant_flow', 'adverse_events', 'outcome_measures_description', 'protocol_link']\n",
    "\n",
    "changes_labels = ['section', 'field_name', 'date', 'reason', 'old_value', 'updated_value']\n",
    "\n",
    "def straight_tables(table_name, label_list):\n",
    "    table = soup.find(text=re.compile(table_name)).parent.parent.parent.find_all('tr')\n",
    "    tab_list = []\n",
    "    for t in table[2:]:\n",
    "        tds = t.find_all('td')\n",
    "        tab_dict = {}\n",
    "        for td, label in zip(tds, label_list):\n",
    "            tab_dict[label] = td.text.strip()\n",
    "        tab_list.append(tab_dict)\n",
    "    return tab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_info(soup):    \n",
    "    t_d = {}\n",
    "    if soup.find(text=re.compile(r'PACTR\\d{15}')):\n",
    "        t_d['trial_id'] = soup.find(text=re.compile(r'PACTR\\d{15}'))\n",
    "        t_d['date_registered'] = soup.find(text=re.compile(r'Date registered')).parent.findNext('td').text.strip()\n",
    "        t_d['registration_status'] = soup.find(text=re.compile(r'Trial Status')).parent.findNext('td').text.strip()\n",
    "\n",
    "        trial_description = soup.find(text=\"TRIAL DESCRIPTION\").parent.parent\n",
    "        tr = trial_description.findNext('tr')\n",
    "        for t_desc in trial_description_labels:\n",
    "            t_d[t_desc] = tr.find('td', {'class':\"info\"}).text.strip()\n",
    "            tr = tr.findNext('tr')\n",
    "\n",
    "        s_i = soup.find(text=re.compile(r'Secondary Ids')).parent.parent.parent.parent\n",
    "        all_ids = s_i.find_all('td', {'class': 'info'})\n",
    "        secondary_ids = []\n",
    "        idx = 0\n",
    "        for n in list(range(0,(int(len(all_ids)/2)))):\n",
    "            id_dict = {}\n",
    "            id_dict['type'] = all_ids[idx].text.strip()\n",
    "            id_dict['id_number'] = all_ids[idx+1].text.strip()\n",
    "            if id_dict['type'] == '' and id_dict['id_number'] == '':\n",
    "                pass\n",
    "            elif id_dict:\n",
    "                secondary_ids.append(id_dict)\n",
    "            else:\n",
    "                pass\n",
    "            idx += 2\n",
    "        t_d['secondary_ids'] = secondary_ids\n",
    "\n",
    "        t_d['study_design'] = straight_tables('STUDY DESIGN', design_labels)\n",
    "\n",
    "        t_d['interventions'] = straight_tables('INTERVENTIONS', int_labels)\n",
    "\n",
    "        t_d['eligibility_criteria'] = straight_tables('ELIGIBILITY CRITERIA', elig_labels)\n",
    "\n",
    "        ethics = soup.find(text=re.compile(r'ETHICS APPROVAL')).parent.parent.parent.find_all('tr')\n",
    "        len_eth = len(ethics)\n",
    "        eth_data_lines = list(range(2,len_eth, 6))\n",
    "        address_lines = list(range(6, len_eth, 6))\n",
    "        ethics_list = []\n",
    "        for line, add_line in zip(eth_data_lines, address_lines):\n",
    "            td = ethics[line].find_all('td')\n",
    "            eth_dict={}\n",
    "            for t, l in zip(td, ethics_labels[:-1]):\n",
    "                eth_dict[l] = t.text.strip()\n",
    "            add = ethics[add_line].find_all('td')\n",
    "            add_dict={}\n",
    "            for tag, lab in zip(add, ethics_address):\n",
    "                add_dict[lab] = tag.text.strip()\n",
    "            eth_dict[ethics_labels[-1]] = add_dict\n",
    "            ethics_list.append(eth_dict)\n",
    "        t_d['ethics_approval'] = ethics_list\n",
    "\n",
    "        t_d['outcomes'] = straight_tables('OUTCOMES', outcome_labels)\n",
    "\n",
    "        t_d['recruitment_centres'] = straight_tables('RECRUITMENT CENTRES', recruit_labels)\n",
    "\n",
    "        t_d['funding_sources'] = straight_tables('FUNDING SOURCES', funding_labels)\n",
    "\n",
    "        t_d['sponsors'] = straight_tables('SPONSORS', spon_labels)\n",
    "\n",
    "        t_d['collaborators'] = straight_tables('COLLABORATORS', collab_labels)\n",
    "\n",
    "        contact = soup.find(text=re.compile(r'CONTACT PEOPLE')).parent.parent.parent.find_all('tr')\n",
    "        cont_lines_1 = list(range(2,len(contact),4))\n",
    "        cont_lines_2 =list(range(4,len(contact),4))\n",
    "        contact_list = []\n",
    "        for line_1, line_2 in zip(cont_lines_1, cont_lines_2):\n",
    "            tds_1 = contact[line_1].find_all('td')\n",
    "            contact_dict = {}\n",
    "            for t_1, lab_1 in zip(tds_1, contact_labels_1):\n",
    "                contact_dict[lab_1] = t_1.text.strip()\n",
    "            tds_2 = contact[line_2].find_all('td')\n",
    "            for t_2, lab_2 in zip(tds_2, contact_labels_2):\n",
    "                contact_dict[lab_2] = t_2.text.strip()\n",
    "            contact_list.append(contact_dict)\n",
    "        t_d['contact_people'] = contact_list\n",
    "\n",
    "        reporting = soup.find(text=re.compile(r'REPORTING')).parent.parent.parent.find_all('tr')\n",
    "        reporting_lines = list(range(2,9,2))\n",
    "        reporting_data = []\n",
    "        for n in reporting_lines:\n",
    "            reporting_data = reporting_data + reporting[n].find_all('td')\n",
    "        results_dict = {}\n",
    "        for d, r in zip(reporting_data, results_labels):\n",
    "            results_dict[r] = d.text.strip()\n",
    "        t_d['reporting'] = results_dict\n",
    "\n",
    "        t_d['trial_history'] = straight_tables('Changes to trial information', changes_labels)\n",
    "\n",
    "        return t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249de7cd03b5432bb822233153ec4613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#there is no good way to get the max trial ID number, based on a search as of 17 February 2020, \n",
    "#it appears that 1 to 11,000 is a relatively safe range. There should be ~2000 registered trial \n",
    "#(2216 as of 17 Feb 2020)\n",
    "\n",
    "def get_url(url):\n",
    "    response = get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "base_url = 'https://pactr.samrc.ac.za/TrialDisplay.aspx?TrialID='\n",
    "max_page = 10000\n",
    "pages = [str(i) for i in range(1,int(max_page)+1)]\n",
    "\n",
    "\n",
    "trial_list = []\n",
    "for page in tqdm(pages):\n",
    "    url = base_url + page\n",
    "    soup = get_url(url)\n",
    "    trial_check = soup.find(text=re.compile(r'Trial no.:'))\n",
    "    if trial_check:\n",
    "        id_check = trial_check.parent.find_next_sibling('td').find(text=re.compile(r'PACTR\\d{15}'))\n",
    "        if id_check:\n",
    "            trial_info = get_trial_info(soup)\n",
    "            trial_list.append(trial_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(trial_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import date\n",
    "\n",
    "def pactr_csv():\n",
    "    with open('pactr- ' + str(date.today()) + '.csv','w', newline = '') as pactr_csv:\n",
    "        writer=csv.writer(pactr_csv)\n",
    "        for val in trial_list:\n",
    "            writer.writerow([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pactr_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
