{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(pages, doc):\n",
    "    new_sufs = []\n",
    "    for page in pages:\n",
    "        form_data = {\n",
    "            \"__EVENTTARGET\": None,\n",
    "            \"ctl00$MainContent$ctrlSearch$hdnSearchType\": \"1\",\n",
    "            \"ctl00$MainContent$ctrlSearch$txtBasicSearch\": \"PACTR\",\n",
    "        }\n",
    "        if len(str(page)) == 1:\n",
    "            page = str(page).zfill(2)\n",
    "        et = \"ctl00$MainContent$ctrlSearch$rptPager$ctl{}$lbPageNumber\"\n",
    "        form_data[\"__EVENTTARGET\"] = et.format(page)\n",
    "        for i in doc.find(\"form\").find_all(\"input\"):\n",
    "            if i[\"id\"].startswith(\"__\"):\n",
    "                form_data[i[\"id\"]] = i[\"value\"]\n",
    "        rsp = session.post(url, form_data)       \n",
    "        soup = BeautifulSoup(rsp.text, \"html.parser\")\n",
    "        for l in soup.find_all('div', {'class':'col-md-6 col-sm-6 text-right resultHeader'}):\n",
    "            new_sufs.append(l.find('a').get('href'))\n",
    "    return new_sufs, soup\n",
    "\n",
    "def next_pages(doc):\n",
    "    # Extract form data for next set of pages\n",
    "    form_data = {\n",
    "        \"__EVENTTARGET\": \"ctl00$MainContent$ctrlSearch$lbNextPage\",\n",
    "        \"ctl00$MainContent$ctrlSearch$hdnSearchType\": \"1\",\n",
    "        \"ctl00$MainContent$ctrlSearch$txtBasicSearch\": \"PACTR\",\n",
    "    }\n",
    "    for i in doc.find(\"form\").find_all(\"input\"):\n",
    "        if i[\"id\"].startswith(\"__\"):\n",
    "            form_data[i[\"id\"]] = i[\"value\"]\n",
    "\n",
    "    # Get next set of pages\n",
    "    rsp = session.post(url, form_data)\n",
    "    doc = BeautifulSoup(rsp.text, \"html.parser\")\n",
    "    return doc\n",
    "\n",
    "def first_page(doc):\n",
    "    suffs = []\n",
    "    # Extract form data\n",
    "    form_data = {\n",
    "        \"__EVENTTARGET\": \"ctl00$MainContent$ctrlSearch$rptPager$ctl00$lbPageNumber\",\n",
    "        \"ctl00$MainContent$ctrlSearch$hdnSearchType\": \"1\",\n",
    "        \"ctl00$MainContent$ctrlSearch$txtBasicSearch\": \"PACTR\",\n",
    "    }\n",
    "    for i in doc.find(\"form\").find_all(\"input\"):\n",
    "        if i[\"id\"].startswith(\"__\"):\n",
    "            form_data[i[\"id\"]] = i[\"value\"]\n",
    "    # Get page\n",
    "    rsp = session.post(url, form_data)\n",
    "    doc = BeautifulSoup(rsp.text, \"html.parser\")\n",
    "\n",
    "    for l in doc.find_all('div', {'class':'col-md-6 col-sm-6 text-right resultHeader'}):\n",
    "        suffs.append(l.find('a').get('href'))\n",
    "    return suffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "session = requests.Session()\n",
    "url_suf = []\n",
    "# Get homepage\n",
    "url = \"https://pactr.samrc.ac.za/Search.aspx\"\n",
    "rsp = session.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract form data\n",
    "doc = BeautifulSoup(rsp.text, \"html.parser\")\n",
    "form_data = {\n",
    "    \"__EVENTTARGET\": \"ctl00$MainContent$ctrlSearch$lbBasicSearch\",\n",
    "    \"ctl00$MainContent$ctrlSearch$hdnSearchType\": \"1\",\n",
    "    \"ctl00$MainContent$ctrlSearch$txtBasicSearch\": \"PACTR\",\n",
    "}\n",
    "for i in doc.find(\"form\").find_all(\"input\"):\n",
    "    if i[\"id\"].startswith(\"__\"):\n",
    "        form_data[i[\"id\"]] = i[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do search (this returns page 1)\n",
    "rsp = session.post(url, form_data)\n",
    "soup = BeautifulSoup(rsp.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in soup.find_all('div', {'class':'col-md-6 col-sm-6 text-right resultHeader'}):\n",
    "    url_suf.append(l.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368297968f834461a192566a2e592e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterations = list(range(1,13))\n",
    "pages = list(range(1,20))\n",
    "\n",
    "counter = 1\n",
    "for i in tqdm(iterations):\n",
    "    if counter < len(iterations):\n",
    "        #this searches a set of pages\n",
    "        output = get_pages(pages, soup)\n",
    "\n",
    "        #take the list output and add it to our overall one\n",
    "        url_suf = url_suf + output[0]\n",
    "\n",
    "        #the new doc to get the next set of pages\n",
    "        doc = output[1]\n",
    "\n",
    "        soup = next_pages(doc)\n",
    "\n",
    "        url_suf = url_suf + first_page(soup)\n",
    "        counter +=1\n",
    "    else:\n",
    "        #This range will be from 1 to 1 + the number of pages on the last set of pages on PACTR\n",
    "        pages = list(range(1,2))\n",
    "        output = get_pages(pages, soup)\n",
    "        url_suf = url_suf + output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2219"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('All URLs: {}'.format(len(url_suf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_urls = list(set(url_suf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2217"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Unique URLs: {}'.format(len(final_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "try:\n",
    "    get_ipython\n",
    "    from tqdm.notebook import tqdm\n",
    "except NameError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "def get_url(url):\n",
    "    response = get(url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_description_labels = ['public_title', 'scientific_title', 'background', 'study_type', 'acronym', 'primary_conditions', \n",
    "                     'secondary_conditions', 'purpose', 'anticipated_start_date', 'actual_start_date', \n",
    "                     'anticipated_completion_date', 'actual_completion_date', 'anticiapted_enrollment', 'actual_enrollment', \n",
    "                     'recruitment_status', 'publication_url']\n",
    "\n",
    "design_labels = ['intervention_assignment', 'allocation_process', 'randomisation_generation', 'allocation_concealment', \n",
    "                 'masking', 'masking_blinding_used']\n",
    "\n",
    "int_labels = ['intervention_type', 'intervention_name', 'dose', 'duration', 'intervention_description', 'group_size',\n",
    "                      'nature_of_control']\n",
    "\n",
    "elig_labels = ['inclusion_criteria', 'exclusion_criteria', 'age_category', 'min_age', 'max_age', 'gender']\n",
    "\n",
    "ethics_labels = ['recieved_ethics', 'date_to_be_submitted_approval', 'date_of_approval', 'ethics_committee', \n",
    "                 'committee_address']\n",
    "\n",
    "ethics_address = ['address', 'city', 'postal_code', 'country']\n",
    "\n",
    "outcome_labels = ['outcome_type', 'outcome', 'measurement_timepoints']\n",
    "\n",
    "recruit_labels = ['centre_name', 'address', 'city', 'postal_code', 'country']\n",
    "\n",
    "funding_labels = ['name_of_funder', 'address', 'city', 'postal_code' 'country']\n",
    "\n",
    "spon_labels = ['sponsor_type', 'sponsor_name', 'address', 'city', 'postal_code', 'country', 'nature_of_sponsor']\n",
    "\n",
    "collab_labels = ['name', 'address', 'city', 'postal_code', 'country']\n",
    "\n",
    "contact_labels_1 = ['role', 'name', 'email', 'phone', 'address']\n",
    "\n",
    "contact_labels_2 = ['city', 'postal_code', 'country', 'position_affiliation']\n",
    "\n",
    "results_labels = ['share_ipd', 'description', 'additiona_documents', 'sharing_time_frame', 'key_access_criteria', 'url',\n",
    "                 'results_available', 'results_summary', 'results_posting_date', 'first_journal_pub_date', 'results_url',\n",
    "                 'baseline_characteristics', 'participant_flow', 'adverse_events', 'outcome_measures_description', 'protocol_link']\n",
    "\n",
    "changes_labels = ['section', 'field_name', 'date', 'reason', 'old_value', 'updated_value']\n",
    "\n",
    "def straight_tables(table_name, label_list):\n",
    "    table = soup.find(text=re.compile(table_name)).parent.parent.parent.find_all('tr')\n",
    "    tab_list = []\n",
    "    for t in table[2:]:\n",
    "        tds = t.find_all('td')\n",
    "        tab_dict = {}\n",
    "        for td, label in zip(tds, label_list):\n",
    "            tab_dict[label] = td.text.strip()\n",
    "        tab_list.append(tab_dict)\n",
    "    return tab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trial_info(soup):    \n",
    "    t_d = {}\n",
    "    if soup.find(text=re.compile(r'PACTR\\d{15}')):\n",
    "        t_d['trial_id'] = soup.find(text=re.compile(r'PACTR\\d{15}'))\n",
    "        t_d['date_registered'] = soup.find(text=re.compile(r'Date registered')).parent.findNext('td').text.strip()\n",
    "        t_d['registration_status'] = soup.find(text=re.compile(r'Trial Status')).parent.findNext('td').text.strip()\n",
    "\n",
    "        trial_description = soup.find(text=\"TRIAL DESCRIPTION\").parent.parent\n",
    "        tr = trial_description.findNext('tr')\n",
    "        for t_desc in trial_description_labels:\n",
    "            t_d[t_desc] = tr.find('td', {'class':\"info\"}).text.strip()\n",
    "            tr = tr.findNext('tr')\n",
    "\n",
    "        s_i = soup.find(text=re.compile(r'Secondary Ids')).parent.parent.parent.parent\n",
    "        all_ids = s_i.find_all('td', {'class': 'info'})\n",
    "        secondary_ids = []\n",
    "        idx = 0\n",
    "        for n in list(range(0,(int(len(all_ids)/2)))):\n",
    "            id_dict = {}\n",
    "            id_dict['type'] = all_ids[idx].text.strip()\n",
    "            id_dict['id_number'] = all_ids[idx+1].text.strip()\n",
    "            if id_dict['type'] == '' and id_dict['id_number'] == '':\n",
    "                pass\n",
    "            elif id_dict:\n",
    "                secondary_ids.append(id_dict)\n",
    "            else:\n",
    "                pass\n",
    "            idx += 2\n",
    "        t_d['secondary_ids'] = secondary_ids\n",
    "\n",
    "        t_d['study_design'] = straight_tables('STUDY DESIGN', design_labels)\n",
    "\n",
    "        t_d['interventions'] = straight_tables('INTERVENTIONS', int_labels)\n",
    "\n",
    "        t_d['eligibility_criteria'] = straight_tables('ELIGIBILITY CRITERIA', elig_labels)\n",
    "\n",
    "        ethics = soup.find(text=re.compile(r'ETHICS APPROVAL')).parent.parent.parent.find_all('tr')\n",
    "        len_eth = len(ethics)\n",
    "        eth_data_lines = list(range(2,len_eth, 6))\n",
    "        address_lines = list(range(6, len_eth, 6))\n",
    "        ethics_list = []\n",
    "        for line, add_line in zip(eth_data_lines, address_lines):\n",
    "            td = ethics[line].find_all('td')\n",
    "            eth_dict={}\n",
    "            for t, l in zip(td, ethics_labels[:-1]):\n",
    "                eth_dict[l] = t.text.strip()\n",
    "            add = ethics[add_line].find_all('td')\n",
    "            add_dict={}\n",
    "            for tag, lab in zip(add, ethics_address):\n",
    "                add_dict[lab] = tag.text.strip()\n",
    "            eth_dict[ethics_labels[-1]] = add_dict\n",
    "            ethics_list.append(eth_dict)\n",
    "        t_d['ethics_approval'] = ethics_list\n",
    "\n",
    "        t_d['outcomes'] = straight_tables('OUTCOMES', outcome_labels)\n",
    "\n",
    "        t_d['recruitment_centres'] = straight_tables('RECRUITMENT CENTRES', recruit_labels)\n",
    "\n",
    "        t_d['funding_sources'] = straight_tables('FUNDING SOURCES', funding_labels)\n",
    "\n",
    "        t_d['sponsors'] = straight_tables('SPONSORS', spon_labels)\n",
    "\n",
    "        t_d['collaborators'] = straight_tables('COLLABORATORS', collab_labels)\n",
    "\n",
    "        contact = soup.find(text=re.compile(r'CONTACT PEOPLE')).parent.parent.parent.find_all('tr')\n",
    "        cont_lines_1 = list(range(2,len(contact),4))\n",
    "        cont_lines_2 =list(range(4,len(contact),4))\n",
    "        contact_list = []\n",
    "        for line_1, line_2 in zip(cont_lines_1, cont_lines_2):\n",
    "            tds_1 = contact[line_1].find_all('td')\n",
    "            contact_dict = {}\n",
    "            for t_1, lab_1 in zip(tds_1, contact_labels_1):\n",
    "                contact_dict[lab_1] = t_1.text.strip()\n",
    "            tds_2 = contact[line_2].find_all('td')\n",
    "            for t_2, lab_2 in zip(tds_2, contact_labels_2):\n",
    "                contact_dict[lab_2] = t_2.text.strip()\n",
    "            contact_list.append(contact_dict)\n",
    "        t_d['contact_people'] = contact_list\n",
    "\n",
    "        reporting = soup.find(text=re.compile(r'REPORTING')).parent.parent.parent.find_all('tr')\n",
    "        reporting_lines = list(range(2,9,2))\n",
    "        reporting_data = []\n",
    "        for n in reporting_lines:\n",
    "            reporting_data = reporting_data + reporting[n].find_all('td')\n",
    "        results_dict = {}\n",
    "        for d, r in zip(reporting_data, results_labels):\n",
    "            results_dict[r] = d.text.strip()\n",
    "        t_d['reporting'] = results_dict\n",
    "\n",
    "        t_d['trial_history'] = straight_tables('Changes to trial information', changes_labels)\n",
    "\n",
    "        return t_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b79c042771e404e86d420fcbeea132b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2217.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://pactr.samrc.ac.za/'\n",
    "\n",
    "trial_list = []\n",
    "for page in tqdm(final_urls):\n",
    "    url = base_url + page\n",
    "    soup = get_url(url)\n",
    "    trial_check = soup.find(text=re.compile(r'Trial no.:'))\n",
    "    if trial_check:\n",
    "        id_check = trial_check.parent.find_next_sibling('td').find(text=re.compile(r'PACTR\\d{15}'))\n",
    "        if id_check:\n",
    "            trial_info = get_trial_info(soup)\n",
    "            trial_list.append(trial_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212\n"
     ]
    }
   ],
   "source": [
    "print('Scraped Trials: {}'.format(len(trial_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ndjson\n",
    "from datetime import date\n",
    "with open('pactr_json_{}.ndjson'.format(date.today()),'w') as r:\n",
    "    ndjson.dump(trial_list, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
